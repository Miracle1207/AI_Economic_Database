{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46778b6b9e2b442c844748ec3281ab77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re \n",
    "import string\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 100\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "tqdm_notebook().pandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, RobustScaler,MinMaxScaler,StandardScaler,LabelEncoder\n",
    "\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, accuracy_score, classification_report, f1_score,recall_score,precision_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eploratory Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "df_merge_clean_posts = pd.read_csv('df_merge_clean_posts.csv')\n",
    "df_merge_tfidf = pd.read_csv('df_merge_tfidf.csv')\n",
    "df_merge_cvn = pd.read_csv('df_merge_cvn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_clean_posts.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df_merge_tfidf.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df_merge_cvn.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_users = df_merge_clean_posts.ptype.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Personality types with one hot encoder\n",
    "The personality type are labeled with digits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "encoder_label = le.fit_transform(df_merge_clean_posts.ptype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_clean_posts.insert(1,'ptype_label',encoder_label)\n",
    "df_merge_tfidf.insert(1,'ptype_label',encoder_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Personality types comparison\n",
    "We can see that the personality types are really unbalanced. \n",
    "It is also observed that the IN++ type personality takes the top 4 spots\n",
    "The majority is INFP at 21.2%. This percentage will be our baseline for accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# number of each personality type\n",
    "plt.figure(figsize=(12,6))\n",
    "temp = df_merge_clean_posts.ptype.value_counts().sort_values(ascending=False)\n",
    "\n",
    "plt.bar(temp.index,temp.values,color=['r','r','r','r','r','r','b','b','b','b','b','b','b','b','b','b'],alpha=0.5)\n",
    "\n",
    "plt.xticks(temp.index,rotation=0,fontsize=12);\n",
    "plt.title('Personality Types Count and Percentages',fontsize=14)\n",
    "plt.ylabel('Count',fontsize=12)\n",
    "plt.xlabel('Perosnality Types',fontsize=12)\n",
    "\n",
    "plt.text(12,temp.values.mean()+50,'mean numbers: {0:.0f} users'.format( temp.values.mean()))\n",
    "plt.axhline(temp.values.mean(),color='r',linestyle='--',label ='mean')\n",
    "\n",
    "for i in range(len(temp.index)):\n",
    "    plt.annotate( '{0:.1f} %'.format(100*temp.values[i]/total_users) ,  # percentage\n",
    "                 (i-.3,temp.values[i]+20))\n",
    "#     plt.annotate( '%',  # percentage\n",
    "#                  (i+.3,temp.values[i]+3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Personality Dichotomy Percentages\n",
    "When we the type dichotomy, the ratio for Extrovert vs Introvert and Intuition vs Sensing is unbalanced (here we defined unbalanced as more than 1:4 ratio or 25%:75%) .  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_E = df_merge_clean_posts.world_E_I.sum()\n",
    "information_S = sum(df_merge_clean_posts.information_S_N)\n",
    "decision_T = sum(df_merge_clean_posts.decision_T_F)\n",
    "structure_J = sum(df_merge_clean_posts.structure_J_P)\n",
    "\n",
    "pchar = {'world_I' : total_users-world_E,\n",
    "         'world_E' : world_E, \n",
    "         'information_N': total_users-information_S,\n",
    "         'information_S' : information_S,\n",
    "         'decision_F': total_users-decision_T, \n",
    "         'decision_T' : decision_T,\n",
    "         'structure_P': total_users-structure_J,\n",
    "         'structure_J' : structure_J, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pchar = pd.DataFrame.from_dict(pchar,orient='index',columns=['pchar'])\n",
    "pchar['percentage'] = pchar.pchar.apply(lambda x: x/total_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pchar</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>world_I</th>\n",
       "      <td>6676</td>\n",
       "      <td>0.769568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world_E</th>\n",
       "      <td>1999</td>\n",
       "      <td>0.230432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>information_N</th>\n",
       "      <td>7478</td>\n",
       "      <td>0.862017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>information_S</th>\n",
       "      <td>1197</td>\n",
       "      <td>0.137983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision_F</th>\n",
       "      <td>4694</td>\n",
       "      <td>0.541095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision_T</th>\n",
       "      <td>3981</td>\n",
       "      <td>0.458905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>structure_P</th>\n",
       "      <td>5241</td>\n",
       "      <td>0.604150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>structure_J</th>\n",
       "      <td>3434</td>\n",
       "      <td>0.395850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               pchar  percentage\n",
       "world_I         6676    0.769568\n",
       "world_E         1999    0.230432\n",
       "information_N   7478    0.862017\n",
       "information_S   1197    0.137983\n",
       "decision_F      4694    0.541095\n",
       "decision_T      3981    0.458905\n",
       "structure_P     5241    0.604150\n",
       "structure_J     3434    0.395850"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pchar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(pchar.index,pchar.pchar,color=['r','r','g','g','b','b','y','y'],alpha=.5)\n",
    "\n",
    "plt.xticks(pchar.index,['Introvert','Extrovert','Intuition','Sensing',\n",
    "                        'Feeling','Thinking','Perceiving','Judging'])\n",
    "plt.title('Personality Dichotomies Percentages')\n",
    "plt.ylabel('Count',fontsize=12)\n",
    "plt.xlabel('Types Dichotomy',fontsize=12)\n",
    "\n",
    "\n",
    "for i in range(len(pchar.index)):\n",
    "    plt.annotate( '{0:.1f} %'.format(100*pchar.percentage[i]) ,  # percentage\n",
    "                 (i-.2,pchar.pchar.values[i]+100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Post comparison\n",
    "We break down the featues in the clean posts dataframe into 3 feature groups \n",
    "1. Subjectivity and Polarity    \n",
    "2. Function words   \n",
    "3. Word counts\n",
    "\n",
    "Unfortuantely, We do not find significance differences accross the personlity types on Subjectivity and Polarity and Word Count. But We do see some features with the function words have a differential range between 5 - 10 %.\n",
    "Intuitively, we probably not see any siginificant accuracy results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subjectivity and Polarity Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ptype', 'ptype_label', 'world_E_I', 'information_S_N', 'decision_T_F',\n",
       "       'structure_J_P', 'clean_posts', 'polarity', 'subjectivity',\n",
       "       'conjunction', 'determiner', 'verb_aux', 'personal_pron',\n",
       "       'possessive_pron', 'verb', 'function_word_count', 'word_count',\n",
       "       'num_of_entries', 'avg_words_post'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge_clean_posts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">polarity</th>\n",
       "      <th colspan=\"2\" halign=\"left\">subjectivity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ptype</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENFJ</th>\n",
       "      <td>0.148847</td>\n",
       "      <td>0.058732</td>\n",
       "      <td>0.538579</td>\n",
       "      <td>0.039702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENFP</th>\n",
       "      <td>0.137147</td>\n",
       "      <td>0.057040</td>\n",
       "      <td>0.538521</td>\n",
       "      <td>0.038671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTJ</th>\n",
       "      <td>0.120511</td>\n",
       "      <td>0.054475</td>\n",
       "      <td>0.525913</td>\n",
       "      <td>0.039602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTP</th>\n",
       "      <td>0.116371</td>\n",
       "      <td>0.051273</td>\n",
       "      <td>0.529699</td>\n",
       "      <td>0.039297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESFJ</th>\n",
       "      <td>0.143048</td>\n",
       "      <td>0.051020</td>\n",
       "      <td>0.533000</td>\n",
       "      <td>0.036801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESFP</th>\n",
       "      <td>0.113083</td>\n",
       "      <td>0.056911</td>\n",
       "      <td>0.535708</td>\n",
       "      <td>0.046290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESTJ</th>\n",
       "      <td>0.117872</td>\n",
       "      <td>0.050423</td>\n",
       "      <td>0.523487</td>\n",
       "      <td>0.043646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESTP</th>\n",
       "      <td>0.118213</td>\n",
       "      <td>0.051019</td>\n",
       "      <td>0.527618</td>\n",
       "      <td>0.045973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFJ</th>\n",
       "      <td>0.125999</td>\n",
       "      <td>0.052026</td>\n",
       "      <td>0.529203</td>\n",
       "      <td>0.038713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFP</th>\n",
       "      <td>0.121611</td>\n",
       "      <td>0.053390</td>\n",
       "      <td>0.532937</td>\n",
       "      <td>0.040903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>0.109291</td>\n",
       "      <td>0.053908</td>\n",
       "      <td>0.521378</td>\n",
       "      <td>0.041478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTP</th>\n",
       "      <td>0.102514</td>\n",
       "      <td>0.053035</td>\n",
       "      <td>0.521051</td>\n",
       "      <td>0.039665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISFJ</th>\n",
       "      <td>0.126283</td>\n",
       "      <td>0.095461</td>\n",
       "      <td>0.534687</td>\n",
       "      <td>0.062397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISFP</th>\n",
       "      <td>0.131952</td>\n",
       "      <td>0.061178</td>\n",
       "      <td>0.533026</td>\n",
       "      <td>0.043422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISTJ</th>\n",
       "      <td>0.114215</td>\n",
       "      <td>0.062272</td>\n",
       "      <td>0.516668</td>\n",
       "      <td>0.038433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISTP</th>\n",
       "      <td>0.106196</td>\n",
       "      <td>0.062485</td>\n",
       "      <td>0.517493</td>\n",
       "      <td>0.053926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       polarity           subjectivity          \n",
       "           mean       std         mean       std\n",
       "ptype                                           \n",
       "ENFJ   0.148847  0.058732     0.538579  0.039702\n",
       "ENFP   0.137147  0.057040     0.538521  0.038671\n",
       "ENTJ   0.120511  0.054475     0.525913  0.039602\n",
       "ENTP   0.116371  0.051273     0.529699  0.039297\n",
       "ESFJ   0.143048  0.051020     0.533000  0.036801\n",
       "ESFP   0.113083  0.056911     0.535708  0.046290\n",
       "ESTJ   0.117872  0.050423     0.523487  0.043646\n",
       "ESTP   0.118213  0.051019     0.527618  0.045973\n",
       "INFJ   0.125999  0.052026     0.529203  0.038713\n",
       "INFP   0.121611  0.053390     0.532937  0.040903\n",
       "INTJ   0.109291  0.053908     0.521378  0.041478\n",
       "INTP   0.102514  0.053035     0.521051  0.039665\n",
       "ISFJ   0.126283  0.095461     0.534687  0.062397\n",
       "ISFP   0.131952  0.061178     0.533026  0.043422\n",
       "ISTJ   0.114215  0.062272     0.516668  0.038433\n",
       "ISTP   0.106196  0.062485     0.517493  0.053926"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge_clean_posts.groupby('ptype')['polarity', 'subjectivity'].agg(['mean','std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function Words comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">conjunction</th>\n",
       "      <th colspan=\"2\" halign=\"left\">determiner</th>\n",
       "      <th colspan=\"2\" halign=\"left\">verb_aux</th>\n",
       "      <th colspan=\"2\" halign=\"left\">personal_pron</th>\n",
       "      <th colspan=\"2\" halign=\"left\">possessive_pron</th>\n",
       "      <th colspan=\"2\" halign=\"left\">verb</th>\n",
       "      <th colspan=\"2\" halign=\"left\">function_word_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ptype</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENFJ</th>\n",
       "      <td>48.615789</td>\n",
       "      <td>16.885696</td>\n",
       "      <td>91.300000</td>\n",
       "      <td>24.415051</td>\n",
       "      <td>17.605263</td>\n",
       "      <td>7.127484</td>\n",
       "      <td>162.110526</td>\n",
       "      <td>50.537360</td>\n",
       "      <td>0.363158</td>\n",
       "      <td>0.809801</td>\n",
       "      <td>79.531579</td>\n",
       "      <td>23.490098</td>\n",
       "      <td>319.994737</td>\n",
       "      <td>86.643312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENFP</th>\n",
       "      <td>48.525926</td>\n",
       "      <td>15.965295</td>\n",
       "      <td>91.118519</td>\n",
       "      <td>25.888271</td>\n",
       "      <td>17.517037</td>\n",
       "      <td>7.071126</td>\n",
       "      <td>156.594074</td>\n",
       "      <td>46.740536</td>\n",
       "      <td>0.447407</td>\n",
       "      <td>0.810615</td>\n",
       "      <td>75.634074</td>\n",
       "      <td>22.284731</td>\n",
       "      <td>314.202963</td>\n",
       "      <td>83.352503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTJ</th>\n",
       "      <td>42.783550</td>\n",
       "      <td>14.581581</td>\n",
       "      <td>93.705628</td>\n",
       "      <td>26.532997</td>\n",
       "      <td>17.787879</td>\n",
       "      <td>7.258760</td>\n",
       "      <td>141.138528</td>\n",
       "      <td>43.793764</td>\n",
       "      <td>0.229437</td>\n",
       "      <td>0.505792</td>\n",
       "      <td>75.787879</td>\n",
       "      <td>23.281697</td>\n",
       "      <td>295.645022</td>\n",
       "      <td>80.820491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTP</th>\n",
       "      <td>43.821898</td>\n",
       "      <td>14.523940</td>\n",
       "      <td>92.310949</td>\n",
       "      <td>25.365825</td>\n",
       "      <td>17.557664</td>\n",
       "      <td>6.797507</td>\n",
       "      <td>139.472993</td>\n",
       "      <td>42.847492</td>\n",
       "      <td>0.335766</td>\n",
       "      <td>0.689068</td>\n",
       "      <td>73.855474</td>\n",
       "      <td>20.695251</td>\n",
       "      <td>293.499270</td>\n",
       "      <td>77.072014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESFJ</th>\n",
       "      <td>48.309524</td>\n",
       "      <td>16.129791</td>\n",
       "      <td>95.071429</td>\n",
       "      <td>28.570318</td>\n",
       "      <td>17.785714</td>\n",
       "      <td>7.915825</td>\n",
       "      <td>161.547619</td>\n",
       "      <td>52.067125</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.655983</td>\n",
       "      <td>76.500000</td>\n",
       "      <td>28.397226</td>\n",
       "      <td>323.071429</td>\n",
       "      <td>98.054339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESFP</th>\n",
       "      <td>39.041667</td>\n",
       "      <td>18.996034</td>\n",
       "      <td>74.500000</td>\n",
       "      <td>30.903419</td>\n",
       "      <td>14.354167</td>\n",
       "      <td>6.892764</td>\n",
       "      <td>123.250000</td>\n",
       "      <td>54.933809</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.445127</td>\n",
       "      <td>61.291667</td>\n",
       "      <td>24.692973</td>\n",
       "      <td>251.333333</td>\n",
       "      <td>102.358222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESTJ</th>\n",
       "      <td>46.384615</td>\n",
       "      <td>19.807781</td>\n",
       "      <td>87.641026</td>\n",
       "      <td>24.942552</td>\n",
       "      <td>17.641026</td>\n",
       "      <td>6.599391</td>\n",
       "      <td>147.564103</td>\n",
       "      <td>47.451246</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.451419</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>21.437793</td>\n",
       "      <td>299.410256</td>\n",
       "      <td>84.634732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESTP</th>\n",
       "      <td>43.977528</td>\n",
       "      <td>17.559624</td>\n",
       "      <td>84.033708</td>\n",
       "      <td>25.433268</td>\n",
       "      <td>16.348315</td>\n",
       "      <td>6.775794</td>\n",
       "      <td>137.303371</td>\n",
       "      <td>47.602856</td>\n",
       "      <td>0.213483</td>\n",
       "      <td>0.553335</td>\n",
       "      <td>72.898876</td>\n",
       "      <td>24.980012</td>\n",
       "      <td>281.876404</td>\n",
       "      <td>86.804616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFJ</th>\n",
       "      <td>47.318367</td>\n",
       "      <td>15.770197</td>\n",
       "      <td>93.783673</td>\n",
       "      <td>25.770063</td>\n",
       "      <td>17.804762</td>\n",
       "      <td>6.884883</td>\n",
       "      <td>155.200680</td>\n",
       "      <td>46.961159</td>\n",
       "      <td>0.368707</td>\n",
       "      <td>0.718406</td>\n",
       "      <td>77.524490</td>\n",
       "      <td>22.618068</td>\n",
       "      <td>314.476190</td>\n",
       "      <td>83.032577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFP</th>\n",
       "      <td>47.437773</td>\n",
       "      <td>16.288143</td>\n",
       "      <td>90.893013</td>\n",
       "      <td>26.270128</td>\n",
       "      <td>17.275109</td>\n",
       "      <td>7.052171</td>\n",
       "      <td>152.191594</td>\n",
       "      <td>47.910004</td>\n",
       "      <td>0.331332</td>\n",
       "      <td>0.777862</td>\n",
       "      <td>74.223799</td>\n",
       "      <td>22.647059</td>\n",
       "      <td>308.128821</td>\n",
       "      <td>85.637034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>42.349221</td>\n",
       "      <td>15.496136</td>\n",
       "      <td>92.029331</td>\n",
       "      <td>28.626943</td>\n",
       "      <td>17.833181</td>\n",
       "      <td>7.501048</td>\n",
       "      <td>136.243813</td>\n",
       "      <td>45.906958</td>\n",
       "      <td>0.267644</td>\n",
       "      <td>0.548432</td>\n",
       "      <td>73.677360</td>\n",
       "      <td>23.556382</td>\n",
       "      <td>288.723190</td>\n",
       "      <td>85.138325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTP</th>\n",
       "      <td>42.873466</td>\n",
       "      <td>15.219754</td>\n",
       "      <td>92.417945</td>\n",
       "      <td>27.480311</td>\n",
       "      <td>17.823620</td>\n",
       "      <td>6.982350</td>\n",
       "      <td>137.625000</td>\n",
       "      <td>45.057426</td>\n",
       "      <td>0.245399</td>\n",
       "      <td>0.549522</td>\n",
       "      <td>74.093558</td>\n",
       "      <td>22.733779</td>\n",
       "      <td>290.985429</td>\n",
       "      <td>82.289275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISFJ</th>\n",
       "      <td>47.240964</td>\n",
       "      <td>17.346445</td>\n",
       "      <td>88.186747</td>\n",
       "      <td>28.881703</td>\n",
       "      <td>17.771084</td>\n",
       "      <td>7.153427</td>\n",
       "      <td>154.542169</td>\n",
       "      <td>53.444233</td>\n",
       "      <td>0.265060</td>\n",
       "      <td>0.552336</td>\n",
       "      <td>77.006024</td>\n",
       "      <td>25.488797</td>\n",
       "      <td>308.006024</td>\n",
       "      <td>97.450687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISFP</th>\n",
       "      <td>43.571956</td>\n",
       "      <td>18.218870</td>\n",
       "      <td>79.808118</td>\n",
       "      <td>27.141060</td>\n",
       "      <td>15.527675</td>\n",
       "      <td>7.120534</td>\n",
       "      <td>140.601476</td>\n",
       "      <td>52.157417</td>\n",
       "      <td>0.261993</td>\n",
       "      <td>0.721106</td>\n",
       "      <td>68.210332</td>\n",
       "      <td>24.768335</td>\n",
       "      <td>279.771218</td>\n",
       "      <td>94.576910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISTJ</th>\n",
       "      <td>42.760976</td>\n",
       "      <td>15.864513</td>\n",
       "      <td>90.785366</td>\n",
       "      <td>28.281893</td>\n",
       "      <td>16.751220</td>\n",
       "      <td>7.511531</td>\n",
       "      <td>143.541463</td>\n",
       "      <td>50.531532</td>\n",
       "      <td>0.248780</td>\n",
       "      <td>0.561476</td>\n",
       "      <td>75.687805</td>\n",
       "      <td>24.961345</td>\n",
       "      <td>294.087805</td>\n",
       "      <td>91.410473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISTP</th>\n",
       "      <td>42.439169</td>\n",
       "      <td>15.731471</td>\n",
       "      <td>88.821958</td>\n",
       "      <td>28.166243</td>\n",
       "      <td>16.267062</td>\n",
       "      <td>7.293173</td>\n",
       "      <td>137.332344</td>\n",
       "      <td>47.608797</td>\n",
       "      <td>0.255193</td>\n",
       "      <td>0.517703</td>\n",
       "      <td>73.192878</td>\n",
       "      <td>24.320312</td>\n",
       "      <td>285.115727</td>\n",
       "      <td>88.237108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      conjunction            determiner              verb_aux            \\\n",
       "             mean        std       mean        std       mean       std   \n",
       "ptype                                                                     \n",
       "ENFJ    48.615789  16.885696  91.300000  24.415051  17.605263  7.127484   \n",
       "ENFP    48.525926  15.965295  91.118519  25.888271  17.517037  7.071126   \n",
       "ENTJ    42.783550  14.581581  93.705628  26.532997  17.787879  7.258760   \n",
       "ENTP    43.821898  14.523940  92.310949  25.365825  17.557664  6.797507   \n",
       "ESFJ    48.309524  16.129791  95.071429  28.570318  17.785714  7.915825   \n",
       "ESFP    39.041667  18.996034  74.500000  30.903419  14.354167  6.892764   \n",
       "ESTJ    46.384615  19.807781  87.641026  24.942552  17.641026  6.599391   \n",
       "ESTP    43.977528  17.559624  84.033708  25.433268  16.348315  6.775794   \n",
       "INFJ    47.318367  15.770197  93.783673  25.770063  17.804762  6.884883   \n",
       "INFP    47.437773  16.288143  90.893013  26.270128  17.275109  7.052171   \n",
       "INTJ    42.349221  15.496136  92.029331  28.626943  17.833181  7.501048   \n",
       "INTP    42.873466  15.219754  92.417945  27.480311  17.823620  6.982350   \n",
       "ISFJ    47.240964  17.346445  88.186747  28.881703  17.771084  7.153427   \n",
       "ISFP    43.571956  18.218870  79.808118  27.141060  15.527675  7.120534   \n",
       "ISTJ    42.760976  15.864513  90.785366  28.281893  16.751220  7.511531   \n",
       "ISTP    42.439169  15.731471  88.821958  28.166243  16.267062  7.293173   \n",
       "\n",
       "      personal_pron            possessive_pron                 verb  \\\n",
       "               mean        std            mean       std       mean   \n",
       "ptype                                                                 \n",
       "ENFJ     162.110526  50.537360        0.363158  0.809801  79.531579   \n",
       "ENFP     156.594074  46.740536        0.447407  0.810615  75.634074   \n",
       "ENTJ     141.138528  43.793764        0.229437  0.505792  75.787879   \n",
       "ENTP     139.472993  42.847492        0.335766  0.689068  73.855474   \n",
       "ESFJ     161.547619  52.067125        0.357143  0.655983  76.500000   \n",
       "ESFP     123.250000  54.933809        0.187500  0.445127  61.291667   \n",
       "ESTJ     147.564103  47.451246        0.179487  0.451419  77.000000   \n",
       "ESTP     137.303371  47.602856        0.213483  0.553335  72.898876   \n",
       "INFJ     155.200680  46.961159        0.368707  0.718406  77.524490   \n",
       "INFP     152.191594  47.910004        0.331332  0.777862  74.223799   \n",
       "INTJ     136.243813  45.906958        0.267644  0.548432  73.677360   \n",
       "INTP     137.625000  45.057426        0.245399  0.549522  74.093558   \n",
       "ISFJ     154.542169  53.444233        0.265060  0.552336  77.006024   \n",
       "ISFP     140.601476  52.157417        0.261993  0.721106  68.210332   \n",
       "ISTJ     143.541463  50.531532        0.248780  0.561476  75.687805   \n",
       "ISTP     137.332344  47.608797        0.255193  0.517703  73.192878   \n",
       "\n",
       "                 function_word_count              \n",
       "             std                mean         std  \n",
       "ptype                                             \n",
       "ENFJ   23.490098          319.994737   86.643312  \n",
       "ENFP   22.284731          314.202963   83.352503  \n",
       "ENTJ   23.281697          295.645022   80.820491  \n",
       "ENTP   20.695251          293.499270   77.072014  \n",
       "ESFJ   28.397226          323.071429   98.054339  \n",
       "ESFP   24.692973          251.333333  102.358222  \n",
       "ESTJ   21.437793          299.410256   84.634732  \n",
       "ESTP   24.980012          281.876404   86.804616  \n",
       "INFJ   22.618068          314.476190   83.032577  \n",
       "INFP   22.647059          308.128821   85.637034  \n",
       "INTJ   23.556382          288.723190   85.138325  \n",
       "INTP   22.733779          290.985429   82.289275  \n",
       "ISFJ   25.488797          308.006024   97.450687  \n",
       "ISFP   24.768335          279.771218   94.576910  \n",
       "ISTJ   24.961345          294.087805   91.410473  \n",
       "ISTP   24.320312          285.115727   88.237108  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge_clean_posts.groupby('ptype')['conjunction', 'determiner', 'verb_aux', 'personal_pron',\n",
    "                                      'possessive_pron', 'verb', 'function_word_count'].agg(['mean','std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Count comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">word_count</th>\n",
       "      <th colspan=\"2\" halign=\"left\">num_of_entries</th>\n",
       "      <th colspan=\"2\" halign=\"left\">avg_words_post</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ptype</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENFJ</th>\n",
       "      <td>1581.584211</td>\n",
       "      <td>378.366610</td>\n",
       "      <td>45.031579</td>\n",
       "      <td>6.683238</td>\n",
       "      <td>34.294737</td>\n",
       "      <td>5.639756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENFP</th>\n",
       "      <td>1557.554074</td>\n",
       "      <td>363.077100</td>\n",
       "      <td>44.785185</td>\n",
       "      <td>6.992986</td>\n",
       "      <td>34.010370</td>\n",
       "      <td>4.996279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTJ</th>\n",
       "      <td>1493.463203</td>\n",
       "      <td>348.737720</td>\n",
       "      <td>44.311688</td>\n",
       "      <td>6.681434</td>\n",
       "      <td>32.952381</td>\n",
       "      <td>4.685072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTP</th>\n",
       "      <td>1480.207299</td>\n",
       "      <td>341.494755</td>\n",
       "      <td>44.702190</td>\n",
       "      <td>5.625917</td>\n",
       "      <td>32.329927</td>\n",
       "      <td>5.264001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESFJ</th>\n",
       "      <td>1580.238095</td>\n",
       "      <td>428.197288</td>\n",
       "      <td>45.190476</td>\n",
       "      <td>7.188195</td>\n",
       "      <td>33.976190</td>\n",
       "      <td>6.361040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESFP</th>\n",
       "      <td>1271.104167</td>\n",
       "      <td>487.806346</td>\n",
       "      <td>40.354167</td>\n",
       "      <td>9.785986</td>\n",
       "      <td>30.229167</td>\n",
       "      <td>7.152561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESTJ</th>\n",
       "      <td>1497.358974</td>\n",
       "      <td>366.553973</td>\n",
       "      <td>44.512821</td>\n",
       "      <td>7.192364</td>\n",
       "      <td>32.923077</td>\n",
       "      <td>4.836183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESTP</th>\n",
       "      <td>1420.224719</td>\n",
       "      <td>392.559600</td>\n",
       "      <td>43.426966</td>\n",
       "      <td>7.704614</td>\n",
       "      <td>31.831461</td>\n",
       "      <td>5.341169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFJ</th>\n",
       "      <td>1566.808844</td>\n",
       "      <td>364.054860</td>\n",
       "      <td>44.540136</td>\n",
       "      <td>6.725946</td>\n",
       "      <td>34.382993</td>\n",
       "      <td>4.966421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFP</th>\n",
       "      <td>1523.703603</td>\n",
       "      <td>376.552935</td>\n",
       "      <td>43.793668</td>\n",
       "      <td>7.188650</td>\n",
       "      <td>33.950873</td>\n",
       "      <td>5.106489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>1464.468378</td>\n",
       "      <td>384.338208</td>\n",
       "      <td>43.521540</td>\n",
       "      <td>7.847997</td>\n",
       "      <td>32.810266</td>\n",
       "      <td>5.126091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTP</th>\n",
       "      <td>1467.889571</td>\n",
       "      <td>369.903798</td>\n",
       "      <td>43.797546</td>\n",
       "      <td>7.319617</td>\n",
       "      <td>32.706288</td>\n",
       "      <td>5.032245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISFJ</th>\n",
       "      <td>1526.084337</td>\n",
       "      <td>436.289248</td>\n",
       "      <td>43.783133</td>\n",
       "      <td>8.511736</td>\n",
       "      <td>33.680723</td>\n",
       "      <td>5.775643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISFP</th>\n",
       "      <td>1400.014760</td>\n",
       "      <td>421.698286</td>\n",
       "      <td>41.785978</td>\n",
       "      <td>8.883940</td>\n",
       "      <td>32.583026</td>\n",
       "      <td>5.695314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISTJ</th>\n",
       "      <td>1490.439024</td>\n",
       "      <td>401.068050</td>\n",
       "      <td>43.185366</td>\n",
       "      <td>8.404293</td>\n",
       "      <td>33.863415</td>\n",
       "      <td>5.728402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISTP</th>\n",
       "      <td>1430.774481</td>\n",
       "      <td>396.769098</td>\n",
       "      <td>43.320475</td>\n",
       "      <td>7.534324</td>\n",
       "      <td>32.097923</td>\n",
       "      <td>5.805316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word_count             num_of_entries           avg_words_post  \\\n",
       "              mean         std           mean       std           mean   \n",
       "ptype                                                                    \n",
       "ENFJ   1581.584211  378.366610      45.031579  6.683238      34.294737   \n",
       "ENFP   1557.554074  363.077100      44.785185  6.992986      34.010370   \n",
       "ENTJ   1493.463203  348.737720      44.311688  6.681434      32.952381   \n",
       "ENTP   1480.207299  341.494755      44.702190  5.625917      32.329927   \n",
       "ESFJ   1580.238095  428.197288      45.190476  7.188195      33.976190   \n",
       "ESFP   1271.104167  487.806346      40.354167  9.785986      30.229167   \n",
       "ESTJ   1497.358974  366.553973      44.512821  7.192364      32.923077   \n",
       "ESTP   1420.224719  392.559600      43.426966  7.704614      31.831461   \n",
       "INFJ   1566.808844  364.054860      44.540136  6.725946      34.382993   \n",
       "INFP   1523.703603  376.552935      43.793668  7.188650      33.950873   \n",
       "INTJ   1464.468378  384.338208      43.521540  7.847997      32.810266   \n",
       "INTP   1467.889571  369.903798      43.797546  7.319617      32.706288   \n",
       "ISFJ   1526.084337  436.289248      43.783133  8.511736      33.680723   \n",
       "ISFP   1400.014760  421.698286      41.785978  8.883940      32.583026   \n",
       "ISTJ   1490.439024  401.068050      43.185366  8.404293      33.863415   \n",
       "ISTP   1430.774481  396.769098      43.320475  7.534324      32.097923   \n",
       "\n",
       "                 \n",
       "            std  \n",
       "ptype            \n",
       "ENFJ   5.639756  \n",
       "ENFP   4.996279  \n",
       "ENTJ   4.685072  \n",
       "ENTP   5.264001  \n",
       "ESFJ   6.361040  \n",
       "ESFP   7.152561  \n",
       "ESTJ   4.836183  \n",
       "ESTP   5.341169  \n",
       "INFJ   4.966421  \n",
       "INFP   5.106489  \n",
       "INTJ   5.126091  \n",
       "INTP   5.032245  \n",
       "ISFJ   5.775643  \n",
       "ISFP   5.695314  \n",
       "ISTJ   5.728402  \n",
       "ISTP   5.805316  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge_clean_posts.groupby('ptype')['word_count','num_of_entries', 'avg_words_post'].agg(['mean','std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling with Personality types\n",
    "\n",
    "Selected Classification Models:\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- K Nearest Neighbours\n",
    "- Naive Bayes \n",
    "\n",
    "Results:\n",
    "- From the selected classifer models with basic hyper tuning, Logistic Regression scored the highest just .1% higher than the next model, Random Forest Classifier.\n",
    "- Accuracy score obtained from clean post dataset score slightly higher than from TF-IDF dataset. This is good news. Because relying on text used is very specific to the content and user's background. On the other hand, the use of pronouns are more encompassing over different topics.\n",
    "- From the confusion, we can clearly see that the best model is leaning towards the majority group, the minority groups are simply ignored. Even within the majority group, the accuracy varies greatly. We could possibly infer that the model is simply 'guessing'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi class scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_score(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    param_grid = dict(n_neighbors=[15,30,50], \n",
    "                      weights=['uniform','distance'], \n",
    "                      metric=['euclidean','manhattan'])\n",
    "    \n",
    "    grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3, scoring='accuracy',return_train_score=True)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print('K Nearest Neighbour best param: ',grid.best_params_)\n",
    "    print('K Nearest Neighbour acc score: {0:.2f} %'.format(grid.best_score_*100))\n",
    "        \n",
    "    \n",
    "    param_grid = dict(penalty =['l1','l2'], \n",
    "                      C= np.logspace(0, 4, 10))\n",
    "                      \n",
    "    grid = GridSearchCV(LogisticRegression(), param_grid, cv=3, scoring='accuracy',return_train_score=True)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print('Logistic Regression best param: ',grid.best_params_)\n",
    "    print('Logistic Regression acc score: {0:.2f} %'.format(grid.best_score_*100))\n",
    "\n",
    "    \n",
    "    param_grid = dict(n_estimators= [100,200],\n",
    "                      max_depth = [5,10],\n",
    "                      max_features= ['auto', 'sqrt', 'log2'])\n",
    "            \n",
    "    grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=3, scoring='accuracy',return_train_score=True)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print('Random Forest best param: ',grid.best_params_)\n",
    "    print('Random Forest acc score: {0:.2f} %'.format(grid.best_score_*100))\n",
    "\n",
    "    \n",
    "    gb = nb.GaussianNB()\n",
    "    gb.fit(X_train,y_train)\n",
    "    pred_y = gb.predict(X_test)\n",
    "    print('Gaussian Naive Bayes score: {0:.2f} %'.format(gb.score(X_test,y_test)*100))\n",
    "    test = gb.predict_proba(X_test) \n",
    "    \n",
    "    bn = nb.BernoulliNB()\n",
    "    bn.fit(X_train,y_train)\n",
    "    pred_y = bn.predict(X_test)\n",
    "    print('Bernoulli Naive Bayes score: {0:.2f} %'.format(bn.score(X_test,y_test)*100))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cm_df(estimater,X_train, X_test, y_train, y_test):\n",
    "    classifier = estimater\n",
    "    classifier.fit(X_train,y_train)\n",
    "\n",
    "    pred_y = classifier.predict_proba(X_test) \n",
    "\n",
    "    x=[]\n",
    "    for i in pred_y:\n",
    "        x.append(i.argmax())\n",
    "\n",
    "    temp=pd.DataFrame(confusion_matrix(y_test,x),columns=le.classes_,index=le.classes_)\n",
    "    temp['total'] = temp.sum(axis=1)\n",
    "    temp['correct'] = np.diagonal(temp)           #correct predictions are along the diagonal\n",
    "    temp['accuracy'] = temp.correct/temp.total\n",
    "    print ('Accuracy score: {}'.format(temp.correct.sum()/temp.total.sum()))\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean Post Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ptype</th>\n",
       "      <th>ptype_label</th>\n",
       "      <th>world_E_I</th>\n",
       "      <th>information_S_N</th>\n",
       "      <th>decision_T_F</th>\n",
       "      <th>structure_J_P</th>\n",
       "      <th>clean_posts</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>conjunction</th>\n",
       "      <th>determiner</th>\n",
       "      <th>verb_aux</th>\n",
       "      <th>personal_pron</th>\n",
       "      <th>possessive_pron</th>\n",
       "      <th>verb</th>\n",
       "      <th>function_word_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>num_of_entries</th>\n",
       "      <th>avg_words_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I got      from what I have read about the enneagram I am a     though  I read somewhere that a ...</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.515</td>\n",
       "      <td>39</td>\n",
       "      <td>137</td>\n",
       "      <td>23</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>399</td>\n",
       "      <td>1835</td>\n",
       "      <td>47</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I am only a mystery to myself hugs from my daughter can not get enough hugs from my wIfe lovIng ...</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.496</td>\n",
       "      <td>38</td>\n",
       "      <td>89</td>\n",
       "      <td>10</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>235</td>\n",
       "      <td>1249</td>\n",
       "      <td>46</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>my frIend was tell a few of us about a guy she had recently slept wIth the guy lIked It when she...</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.471</td>\n",
       "      <td>35</td>\n",
       "      <td>85</td>\n",
       "      <td>7</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>249</td>\n",
       "      <td>1203</td>\n",
       "      <td>46</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>nobody Is the same so realIstIcally It Is     ImpossIble to base anyone off of stereotypes every...</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.565</td>\n",
       "      <td>57</td>\n",
       "      <td>115</td>\n",
       "      <td>13</td>\n",
       "      <td>126</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>311</td>\n",
       "      <td>1626</td>\n",
       "      <td>47</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I do not offer advIce unless It Is asked for or specIfIcally requested and I expect people to do...</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.524</td>\n",
       "      <td>27</td>\n",
       "      <td>85</td>\n",
       "      <td>14</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>266</td>\n",
       "      <td>1288</td>\n",
       "      <td>44</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I know thIs feelIng bro for example today I went to an awesome bIkIng track wIth small hIlls her...</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.577</td>\n",
       "      <td>55</td>\n",
       "      <td>91</td>\n",
       "      <td>14</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>306</td>\n",
       "      <td>1483</td>\n",
       "      <td>44</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sIr are you hIgh wow you are pretty sucessfull person beIng ceo wIth     employees are quIte som...</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.551</td>\n",
       "      <td>53</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>270</td>\n",
       "      <td>1573</td>\n",
       "      <td>47</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>raInquIlIty  w  w  w  the thInker spso the castle defender vIrgo   w  w  w  the ambassador sxsp ...</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.547</td>\n",
       "      <td>17</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>108</td>\n",
       "      <td>650</td>\n",
       "      <td>32</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>welcome to the forums  and  are verrrrry dIfferent types luckIly there Is a whole bunch of resou...</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.590</td>\n",
       "      <td>41</td>\n",
       "      <td>73</td>\n",
       "      <td>12</td>\n",
       "      <td>124</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>250</td>\n",
       "      <td>1364</td>\n",
       "      <td>47</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>started learnIng how to surf a few months ago d   Insert complIcated solutIon to a problem her...</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.519</td>\n",
       "      <td>44</td>\n",
       "      <td>112</td>\n",
       "      <td>17</td>\n",
       "      <td>169</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>342</td>\n",
       "      <td>1777</td>\n",
       "      <td>46</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Is a thIef when you are undecIded</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>looks lIke you love takIng that test lmao I do too some people on thIs websIte are really scary ...</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.554</td>\n",
       "      <td>35</td>\n",
       "      <td>56</td>\n",
       "      <td>9</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>189</td>\n",
       "      <td>978</td>\n",
       "      <td>38</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>amerIca Is so  you wIll comply I thInk understandIng and acceptance are more Important than type...</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.561</td>\n",
       "      <td>49</td>\n",
       "      <td>109</td>\n",
       "      <td>14</td>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>320</td>\n",
       "      <td>1502</td>\n",
       "      <td>45</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>same here so curIous haha dItto the pIcture above Is my male versIon of that   an ex of mIne who...</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.491</td>\n",
       "      <td>34</td>\n",
       "      <td>80</td>\n",
       "      <td>17</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>245</td>\n",
       "      <td>1265</td>\n",
       "      <td>44</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>mhmmm typIng someone by the way they wrIte InterestIng and probably Irrelevant at some level I d...</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.497</td>\n",
       "      <td>41</td>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "      <td>156</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>322</td>\n",
       "      <td>1622</td>\n",
       "      <td>47</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I am usually about average at games and thats after a huge amount of tIme Invested In them I wou...</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.491</td>\n",
       "      <td>41</td>\n",
       "      <td>92</td>\n",
       "      <td>29</td>\n",
       "      <td>189</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>351</td>\n",
       "      <td>1584</td>\n",
       "      <td>49</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you beat It wIth a stIck fIxed tongue  I saw dIsneys tarzan not too long ago fluffy story asIdeo...</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.525</td>\n",
       "      <td>39</td>\n",
       "      <td>77</td>\n",
       "      <td>11</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>240</td>\n",
       "      <td>1195</td>\n",
       "      <td>46</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>deleted thread wrItten whIle drunk  thats how come you got Into a chItchat In the aIrport securI...</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.532</td>\n",
       "      <td>46</td>\n",
       "      <td>86</td>\n",
       "      <td>14</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>257</td>\n",
       "      <td>1352</td>\n",
       "      <td>39</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>more   s funk       nasa cassInI spacecraft In orbIt around saturn took a pIcture of the earth f...</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.436</td>\n",
       "      <td>28</td>\n",
       "      <td>81</td>\n",
       "      <td>10</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>176</td>\n",
       "      <td>912</td>\n",
       "      <td>37</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I journal only In tImes of severe depressIon though I tend to want to  for some reason  wrIte It...</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.523</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>23</td>\n",
       "      <td>223</td>\n",
       "      <td>2</td>\n",
       "      <td>101</td>\n",
       "      <td>428</td>\n",
       "      <td>2077</td>\n",
       "      <td>50</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>please do not leave me a bunch of pIty more than anythIng I just felt the need to wrIte my thoug...</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.462</td>\n",
       "      <td>35</td>\n",
       "      <td>126</td>\n",
       "      <td>22</td>\n",
       "      <td>186</td>\n",
       "      <td>1</td>\n",
       "      <td>119</td>\n",
       "      <td>370</td>\n",
       "      <td>1737</td>\n",
       "      <td>49</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>If I am uncertaIn about somethIng then It means I am subconscIously or conscIously abIdIng to a ...</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.561</td>\n",
       "      <td>53</td>\n",
       "      <td>106</td>\n",
       "      <td>33</td>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "      <td>337</td>\n",
       "      <td>1643</td>\n",
       "      <td>48</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>It Is been a whIle guys also I bleached my haIr why are people so crazy I absolutely loat...</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.541</td>\n",
       "      <td>15</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>117</td>\n",
       "      <td>657</td>\n",
       "      <td>33</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I was really close frIends wIth an  In college untIl some stuff tore us apart In our frIend grou...</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.517</td>\n",
       "      <td>52</td>\n",
       "      <td>95</td>\n",
       "      <td>19</td>\n",
       "      <td>193</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>359</td>\n",
       "      <td>1722</td>\n",
       "      <td>50</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>po sIr could I ave some more monkey gone to heaven Is lIke the only optIon anyone should choose ...</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.613</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>57</td>\n",
       "      <td>281</td>\n",
       "      <td>20</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I lIke to sIng but I do not thInk It Is because of our personalIty type unsure when I took the p...</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.580</td>\n",
       "      <td>29</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>285</td>\n",
       "      <td>1483</td>\n",
       "      <td>48</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I guess I was curIous about the modus operandI of an  whenever she Is doIng thIs partIcular kInd...</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.561</td>\n",
       "      <td>76</td>\n",
       "      <td>147</td>\n",
       "      <td>18</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>393</td>\n",
       "      <td>1775</td>\n",
       "      <td>47</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>could I have my name changed to barkhouse thank you thank you for postIng thIs   never have I se...</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.359</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>212</td>\n",
       "      <td>7</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>haha that must have been It I need to come back and chat sometIme  I want to dIscuss type stuuuu...</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.569</td>\n",
       "      <td>20</td>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>142</td>\n",
       "      <td>815</td>\n",
       "      <td>33</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>for gettIng routInes done these artIcles have some good poInts      I just saw few days ago an e...</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.545</td>\n",
       "      <td>58</td>\n",
       "      <td>98</td>\n",
       "      <td>7</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>279</td>\n",
       "      <td>1545</td>\n",
       "      <td>40</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7904</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I agree wIth them all except       I cant stand the too fast too furIous movIes and I dont care ...</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.550</td>\n",
       "      <td>45</td>\n",
       "      <td>101</td>\n",
       "      <td>9</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>271</td>\n",
       "      <td>1531</td>\n",
       "      <td>50</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7939</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I do have glasses and my hands are pretty haIry   waIt what kInd of over stImulatIon are we tal...</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.555</td>\n",
       "      <td>57</td>\n",
       "      <td>132</td>\n",
       "      <td>19</td>\n",
       "      <td>119</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>328</td>\n",
       "      <td>1786</td>\n",
       "      <td>48</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7945</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>there Is somethIng IronIc about s beIng In a posItIon of authorIty or makIng others respect rule...</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.537</td>\n",
       "      <td>26</td>\n",
       "      <td>50</td>\n",
       "      <td>12</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>211</td>\n",
       "      <td>1015</td>\n",
       "      <td>39</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7973</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>thanks for clarIfyIng your questIon has been answered the lack of mIle hIgh club optIon Is dIsap...</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.495</td>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>29</td>\n",
       "      <td>148</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>321</td>\n",
       "      <td>1444</td>\n",
       "      <td>47</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>havIng some physIcal actIon In my lIfe helps to revItalIze me and reconnect me wIth the tangIble...</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.483</td>\n",
       "      <td>44</td>\n",
       "      <td>95</td>\n",
       "      <td>21</td>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>308</td>\n",
       "      <td>1616</td>\n",
       "      <td>48</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8040</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I thInk generalIsed lIsts are not a good approach  we are all dIfferent personalty types and hav...</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.480</td>\n",
       "      <td>32</td>\n",
       "      <td>87</td>\n",
       "      <td>17</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>281</td>\n",
       "      <td>1301</td>\n",
       "      <td>46</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8139</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I am good wIth tools but I do not own a car I am alrIght wIth my hands It Is all about context a...</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.496</td>\n",
       "      <td>53</td>\n",
       "      <td>108</td>\n",
       "      <td>20</td>\n",
       "      <td>211</td>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "      <td>393</td>\n",
       "      <td>1939</td>\n",
       "      <td>50</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8192</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>well on my prevIous workplace I had a boss Instead he was actually pretty good  and  just dIffer...</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.507</td>\n",
       "      <td>41</td>\n",
       "      <td>94</td>\n",
       "      <td>13</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>265</td>\n",
       "      <td>1333</td>\n",
       "      <td>40</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8223</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>space to move around In just a sex customer  It Is worrIsome work you have to own It I lIke essa...</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.503</td>\n",
       "      <td>34</td>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>241</td>\n",
       "      <td>1214</td>\n",
       "      <td>44</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8228</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I have dated an  and an    our communIcatIon styles where drastIcally dIfferent on a regular bas...</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.452</td>\n",
       "      <td>31</td>\n",
       "      <td>93</td>\n",
       "      <td>16</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>318</td>\n",
       "      <td>1421</td>\n",
       "      <td>47</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8295</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>banned for hIdIng alcohol In that cup shocked banned because god knows what are you up wIth that...</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.629</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>70</td>\n",
       "      <td>360</td>\n",
       "      <td>12</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8298</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>so thIs sounds really famIlIar to me I was In a relatIonshIp wIth an  for a year and we had the ...</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.495</td>\n",
       "      <td>59</td>\n",
       "      <td>134</td>\n",
       "      <td>25</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>454</td>\n",
       "      <td>2069</td>\n",
       "      <td>50</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8299</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>edIt I now realIze thIs thread Is for what we are wearIng not the actual habIts sooooo yeah  sle...</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.570</td>\n",
       "      <td>50</td>\n",
       "      <td>84</td>\n",
       "      <td>15</td>\n",
       "      <td>139</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>289</td>\n",
       "      <td>1472</td>\n",
       "      <td>48</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8300</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>do not know over the edge sometImes  It Is called an adventure haha done It year years ago the h...</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.469</td>\n",
       "      <td>41</td>\n",
       "      <td>111</td>\n",
       "      <td>13</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>261</td>\n",
       "      <td>1493</td>\n",
       "      <td>45</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8301</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I happen to be a helIcopter mechanIc but where the  comes In Is the constant thought and analysI...</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.468</td>\n",
       "      <td>41</td>\n",
       "      <td>101</td>\n",
       "      <td>5</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>66</td>\n",
       "      <td>246</td>\n",
       "      <td>1294</td>\n",
       "      <td>46</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8308</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>wrong  I joIned yclub academIc team and also ran track and I occasIonally dId xc rugby and socce...</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.570</td>\n",
       "      <td>36</td>\n",
       "      <td>67</td>\n",
       "      <td>9</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>186</td>\n",
       "      <td>1144</td>\n",
       "      <td>42</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8324</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>for guys It Is more about hIerarchy and not so much about your psychologIcalemotIonal makeup the...</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.533</td>\n",
       "      <td>43</td>\n",
       "      <td>78</td>\n",
       "      <td>14</td>\n",
       "      <td>119</td>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>255</td>\n",
       "      <td>1240</td>\n",
       "      <td>37</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8326</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>hey so IncIdentally that Is the exact same thIng that happened to me too It was the same order  ...</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.509</td>\n",
       "      <td>43</td>\n",
       "      <td>84</td>\n",
       "      <td>19</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>344</td>\n",
       "      <td>1668</td>\n",
       "      <td>49</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8373</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I recently remade my lastfm not that thIs wIll be much better  I am Into pop punk posthardcore I...</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.490</td>\n",
       "      <td>43</td>\n",
       "      <td>81</td>\n",
       "      <td>10</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>264</td>\n",
       "      <td>1310</td>\n",
       "      <td>30</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8416</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>what Is your opInIon on them theIr sIde effects It Is gettIng to the poInt where I can not handl...</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.612</td>\n",
       "      <td>38</td>\n",
       "      <td>99</td>\n",
       "      <td>13</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>307</td>\n",
       "      <td>1593</td>\n",
       "      <td>49</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8419</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I agree wIth seralya message hIm that and then just waIt he wIll read It then Itll take hIm some...</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.511</td>\n",
       "      <td>31</td>\n",
       "      <td>61</td>\n",
       "      <td>9</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>214</td>\n",
       "      <td>950</td>\n",
       "      <td>31</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8439</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you know you want to try It  I do I would watched the ted talk before but dId agaIn for good m...</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.436</td>\n",
       "      <td>39</td>\n",
       "      <td>137</td>\n",
       "      <td>22</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>302</td>\n",
       "      <td>1675</td>\n",
       "      <td>45</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8496</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>not even an  but you are god damn rIght there Is not much better than InducIng an orgasm for the...</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.561</td>\n",
       "      <td>24</td>\n",
       "      <td>88</td>\n",
       "      <td>18</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>225</td>\n",
       "      <td>1249</td>\n",
       "      <td>42</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8519</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>nah I am lIke      I was not emotIonal when I lost weIght not lIke In sad way on the contrary I ...</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.530</td>\n",
       "      <td>13</td>\n",
       "      <td>66</td>\n",
       "      <td>9</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>161</td>\n",
       "      <td>857</td>\n",
       "      <td>37</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8533</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>just put them In front of the tv or gIve them a tablet to play wIth kIds nowadays make them some...</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.471</td>\n",
       "      <td>56</td>\n",
       "      <td>89</td>\n",
       "      <td>18</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>338</td>\n",
       "      <td>1642</td>\n",
       "      <td>48</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8541</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I got  my fInal word was bum so I guess that counts for somethIn I am not sure what but sumthIn ...</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.500</td>\n",
       "      <td>27</td>\n",
       "      <td>72</td>\n",
       "      <td>3</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>192</td>\n",
       "      <td>1024</td>\n",
       "      <td>45</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8555</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I dont mInd If ppl do sleep whIt socks  when I was young just a lIttle boy I dId that to  but la...</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.577</td>\n",
       "      <td>43</td>\n",
       "      <td>56</td>\n",
       "      <td>8</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>224</td>\n",
       "      <td>1155</td>\n",
       "      <td>49</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8583</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I was In the navy for   years  I hated every mInute of It  I am glad I dId It now  I wIsh I woul...</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.485</td>\n",
       "      <td>25</td>\n",
       "      <td>76</td>\n",
       "      <td>15</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>235</td>\n",
       "      <td>1184</td>\n",
       "      <td>41</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8628</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>to me to be sensItIve Is to be human  I have a ton of soft spots  the maIn thIng Is that I norma...</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.513</td>\n",
       "      <td>64</td>\n",
       "      <td>121</td>\n",
       "      <td>21</td>\n",
       "      <td>142</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>348</td>\n",
       "      <td>1877</td>\n",
       "      <td>49</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8651</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>beIng alone beIng InsensItIve  over thInkIng out loud womens earnIngs and Income  catalyst   any...</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.523</td>\n",
       "      <td>56</td>\n",
       "      <td>113</td>\n",
       "      <td>25</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>339</td>\n",
       "      <td>1543</td>\n",
       "      <td>43</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>337 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ptype  ptype_label  world_E_I  information_S_N  decision_T_F  \\\n",
       "40    ISTP           15          0                1             1   \n",
       "73    ISTP           15          0                1             1   \n",
       "104   ISTP           15          0                1             1   \n",
       "114   ISTP           15          0                1             1   \n",
       "123   ISTP           15          0                1             1   \n",
       "174   ISTP           15          0                1             1   \n",
       "188   ISTP           15          0                1             1   \n",
       "236   ISTP           15          0                1             1   \n",
       "254   ISTP           15          0                1             1   \n",
       "303   ISTP           15          0                1             1   \n",
       "382   ISTP           15          0                1             1   \n",
       "431   ISTP           15          0                1             1   \n",
       "442   ISTP           15          0                1             1   \n",
       "463   ISTP           15          0                1             1   \n",
       "474   ISTP           15          0                1             1   \n",
       "478   ISTP           15          0                1             1   \n",
       "509   ISTP           15          0                1             1   \n",
       "523   ISTP           15          0                1             1   \n",
       "567   ISTP           15          0                1             1   \n",
       "570   ISTP           15          0                1             1   \n",
       "607   ISTP           15          0                1             1   \n",
       "621   ISTP           15          0                1             1   \n",
       "635   ISTP           15          0                1             1   \n",
       "638   ISTP           15          0                1             1   \n",
       "665   ISTP           15          0                1             1   \n",
       "670   ISTP           15          0                1             1   \n",
       "678   ISTP           15          0                1             1   \n",
       "736   ISTP           15          0                1             1   \n",
       "744   ISTP           15          0                1             1   \n",
       "751   ISTP           15          0                1             1   \n",
       "...    ...          ...        ...              ...           ...   \n",
       "7904  ISTP           15          0                1             1   \n",
       "7939  ISTP           15          0                1             1   \n",
       "7945  ISTP           15          0                1             1   \n",
       "7973  ISTP           15          0                1             1   \n",
       "7995  ISTP           15          0                1             1   \n",
       "8040  ISTP           15          0                1             1   \n",
       "8139  ISTP           15          0                1             1   \n",
       "8192  ISTP           15          0                1             1   \n",
       "8223  ISTP           15          0                1             1   \n",
       "8228  ISTP           15          0                1             1   \n",
       "8295  ISTP           15          0                1             1   \n",
       "8298  ISTP           15          0                1             1   \n",
       "8299  ISTP           15          0                1             1   \n",
       "8300  ISTP           15          0                1             1   \n",
       "8301  ISTP           15          0                1             1   \n",
       "8308  ISTP           15          0                1             1   \n",
       "8324  ISTP           15          0                1             1   \n",
       "8326  ISTP           15          0                1             1   \n",
       "8373  ISTP           15          0                1             1   \n",
       "8416  ISTP           15          0                1             1   \n",
       "8419  ISTP           15          0                1             1   \n",
       "8439  ISTP           15          0                1             1   \n",
       "8496  ISTP           15          0                1             1   \n",
       "8519  ISTP           15          0                1             1   \n",
       "8533  ISTP           15          0                1             1   \n",
       "8541  ISTP           15          0                1             1   \n",
       "8555  ISTP           15          0                1             1   \n",
       "8583  ISTP           15          0                1             1   \n",
       "8628  ISTP           15          0                1             1   \n",
       "8651  ISTP           15          0                1             1   \n",
       "\n",
       "      structure_J_P  \\\n",
       "40                0   \n",
       "73                0   \n",
       "104               0   \n",
       "114               0   \n",
       "123               0   \n",
       "174               0   \n",
       "188               0   \n",
       "236               0   \n",
       "254               0   \n",
       "303               0   \n",
       "382               0   \n",
       "431               0   \n",
       "442               0   \n",
       "463               0   \n",
       "474               0   \n",
       "478               0   \n",
       "509               0   \n",
       "523               0   \n",
       "567               0   \n",
       "570               0   \n",
       "607               0   \n",
       "621               0   \n",
       "635               0   \n",
       "638               0   \n",
       "665               0   \n",
       "670               0   \n",
       "678               0   \n",
       "736               0   \n",
       "744               0   \n",
       "751               0   \n",
       "...             ...   \n",
       "7904              0   \n",
       "7939              0   \n",
       "7945              0   \n",
       "7973              0   \n",
       "7995              0   \n",
       "8040              0   \n",
       "8139              0   \n",
       "8192              0   \n",
       "8223              0   \n",
       "8228              0   \n",
       "8295              0   \n",
       "8298              0   \n",
       "8299              0   \n",
       "8300              0   \n",
       "8301              0   \n",
       "8308              0   \n",
       "8324              0   \n",
       "8326              0   \n",
       "8373              0   \n",
       "8416              0   \n",
       "8419              0   \n",
       "8439              0   \n",
       "8496              0   \n",
       "8519              0   \n",
       "8533              0   \n",
       "8541              0   \n",
       "8555              0   \n",
       "8583              0   \n",
       "8628              0   \n",
       "8651              0   \n",
       "\n",
       "                                                                                              clean_posts  \\\n",
       "40    I got      from what I have read about the enneagram I am a     though  I read somewhere that a ...   \n",
       "73    I am only a mystery to myself hugs from my daughter can not get enough hugs from my wIfe lovIng ...   \n",
       "104   my frIend was tell a few of us about a guy she had recently slept wIth the guy lIked It when she...   \n",
       "114   nobody Is the same so realIstIcally It Is     ImpossIble to base anyone off of stereotypes every...   \n",
       "123   I do not offer advIce unless It Is asked for or specIfIcally requested and I expect people to do...   \n",
       "174   I know thIs feelIng bro for example today I went to an awesome bIkIng track wIth small hIlls her...   \n",
       "188   sIr are you hIgh wow you are pretty sucessfull person beIng ceo wIth     employees are quIte som...   \n",
       "236   raInquIlIty  w  w  w  the thInker spso the castle defender vIrgo   w  w  w  the ambassador sxsp ...   \n",
       "254   welcome to the forums  and  are verrrrry dIfferent types luckIly there Is a whole bunch of resou...   \n",
       "303     started learnIng how to surf a few months ago d   Insert complIcated solutIon to a problem her...   \n",
       "382                                                                     Is a thIef when you are undecIded   \n",
       "431   looks lIke you love takIng that test lmao I do too some people on thIs websIte are really scary ...   \n",
       "442   amerIca Is so  you wIll comply I thInk understandIng and acceptance are more Important than type...   \n",
       "463   same here so curIous haha dItto the pIcture above Is my male versIon of that   an ex of mIne who...   \n",
       "474   mhmmm typIng someone by the way they wrIte InterestIng and probably Irrelevant at some level I d...   \n",
       "478   I am usually about average at games and thats after a huge amount of tIme Invested In them I wou...   \n",
       "509   you beat It wIth a stIck fIxed tongue  I saw dIsneys tarzan not too long ago fluffy story asIdeo...   \n",
       "523   deleted thread wrItten whIle drunk  thats how come you got Into a chItchat In the aIrport securI...   \n",
       "567   more   s funk       nasa cassInI spacecraft In orbIt around saturn took a pIcture of the earth f...   \n",
       "570   I journal only In tImes of severe depressIon though I tend to want to  for some reason  wrIte It...   \n",
       "607   please do not leave me a bunch of pIty more than anythIng I just felt the need to wrIte my thoug...   \n",
       "621   If I am uncertaIn about somethIng then It means I am subconscIously or conscIously abIdIng to a ...   \n",
       "635          It Is been a whIle guys also I bleached my haIr why are people so crazy I absolutely loat...   \n",
       "638   I was really close frIends wIth an  In college untIl some stuff tore us apart In our frIend grou...   \n",
       "665   po sIr could I ave some more monkey gone to heaven Is lIke the only optIon anyone should choose ...   \n",
       "670   I lIke to sIng but I do not thInk It Is because of our personalIty type unsure when I took the p...   \n",
       "678   I guess I was curIous about the modus operandI of an  whenever she Is doIng thIs partIcular kInd...   \n",
       "736   could I have my name changed to barkhouse thank you thank you for postIng thIs   never have I se...   \n",
       "744   haha that must have been It I need to come back and chat sometIme  I want to dIscuss type stuuuu...   \n",
       "751   for gettIng routInes done these artIcles have some good poInts      I just saw few days ago an e...   \n",
       "...                                                                                                   ...   \n",
       "7904  I agree wIth them all except       I cant stand the too fast too furIous movIes and I dont care ...   \n",
       "7939   I do have glasses and my hands are pretty haIry   waIt what kInd of over stImulatIon are we tal...   \n",
       "7945  there Is somethIng IronIc about s beIng In a posItIon of authorIty or makIng others respect rule...   \n",
       "7973  thanks for clarIfyIng your questIon has been answered the lack of mIle hIgh club optIon Is dIsap...   \n",
       "7995  havIng some physIcal actIon In my lIfe helps to revItalIze me and reconnect me wIth the tangIble...   \n",
       "8040  I thInk generalIsed lIsts are not a good approach  we are all dIfferent personalty types and hav...   \n",
       "8139  I am good wIth tools but I do not own a car I am alrIght wIth my hands It Is all about context a...   \n",
       "8192  well on my prevIous workplace I had a boss Instead he was actually pretty good  and  just dIffer...   \n",
       "8223  space to move around In just a sex customer  It Is worrIsome work you have to own It I lIke essa...   \n",
       "8228  I have dated an  and an    our communIcatIon styles where drastIcally dIfferent on a regular bas...   \n",
       "8295  banned for hIdIng alcohol In that cup shocked banned because god knows what are you up wIth that...   \n",
       "8298  so thIs sounds really famIlIar to me I was In a relatIonshIp wIth an  for a year and we had the ...   \n",
       "8299  edIt I now realIze thIs thread Is for what we are wearIng not the actual habIts sooooo yeah  sle...   \n",
       "8300  do not know over the edge sometImes  It Is called an adventure haha done It year years ago the h...   \n",
       "8301  I happen to be a helIcopter mechanIc but where the  comes In Is the constant thought and analysI...   \n",
       "8308  wrong  I joIned yclub academIc team and also ran track and I occasIonally dId xc rugby and socce...   \n",
       "8324  for guys It Is more about hIerarchy and not so much about your psychologIcalemotIonal makeup the...   \n",
       "8326  hey so IncIdentally that Is the exact same thIng that happened to me too It was the same order  ...   \n",
       "8373  I recently remade my lastfm not that thIs wIll be much better  I am Into pop punk posthardcore I...   \n",
       "8416  what Is your opInIon on them theIr sIde effects It Is gettIng to the poInt where I can not handl...   \n",
       "8419  I agree wIth seralya message hIm that and then just waIt he wIll read It then Itll take hIm some...   \n",
       "8439    you know you want to try It  I do I would watched the ted talk before but dId agaIn for good m...   \n",
       "8496  not even an  but you are god damn rIght there Is not much better than InducIng an orgasm for the...   \n",
       "8519  nah I am lIke      I was not emotIonal when I lost weIght not lIke In sad way on the contrary I ...   \n",
       "8533  just put them In front of the tv or gIve them a tablet to play wIth kIds nowadays make them some...   \n",
       "8541  I got  my fInal word was bum so I guess that counts for somethIn I am not sure what but sumthIn ...   \n",
       "8555  I dont mInd If ppl do sleep whIt socks  when I was young just a lIttle boy I dId that to  but la...   \n",
       "8583  I was In the navy for   years  I hated every mInute of It  I am glad I dId It now  I wIsh I woul...   \n",
       "8628  to me to be sensItIve Is to be human  I have a ton of soft spots  the maIn thIng Is that I norma...   \n",
       "8651  beIng alone beIng InsensItIve  over thInkIng out loud womens earnIngs and Income  catalyst   any...   \n",
       "\n",
       "      polarity  subjectivity  conjunction  determiner  verb_aux  \\\n",
       "40       0.161         0.515           39         137        23   \n",
       "73       0.133         0.496           38          89        10   \n",
       "104      0.095         0.471           35          85         7   \n",
       "114      0.092         0.565           57         115        13   \n",
       "123      0.072         0.524           27          85        14   \n",
       "174      0.163         0.577           55          91        14   \n",
       "188      0.082         0.551           53          67        12   \n",
       "236      0.156         0.547           17          29         7   \n",
       "254      0.075         0.590           41          73        12   \n",
       "303      0.079         0.519           44         112        17   \n",
       "382      0.000         0.000            0           1         0   \n",
       "431      0.095         0.554           35          56         9   \n",
       "442      0.094         0.561           49         109        14   \n",
       "463      0.150         0.491           34          80        17   \n",
       "474      0.202         0.497           41         100        25   \n",
       "478      0.064         0.491           41          92        29   \n",
       "509      0.096         0.525           39          77        11   \n",
       "523      0.099         0.532           46          86        14   \n",
       "567      0.186         0.436           28          81        10   \n",
       "570      0.073         0.523           80         100        23   \n",
       "607      0.080         0.462           35         126        22   \n",
       "621      0.031         0.561           53         106        33   \n",
       "635      0.027         0.541           15          38         7   \n",
       "638      0.015         0.517           52          95        19   \n",
       "665      0.135         0.613            7          26         5   \n",
       "670      0.176         0.580           29          72        23   \n",
       "678      0.142         0.561           76         147        18   \n",
       "736      0.112         0.359            7          11         5   \n",
       "744      0.147         0.569           20          45         5   \n",
       "751      0.134         0.545           58          98         7   \n",
       "...        ...           ...          ...         ...       ...   \n",
       "7904     0.203         0.550           45         101         9   \n",
       "7939     0.050         0.555           57         132        19   \n",
       "7945     0.140         0.537           26          50        12   \n",
       "7973     0.137         0.495           45          98        29   \n",
       "7995     0.086         0.483           44          95        21   \n",
       "8040     0.165         0.480           32          87        17   \n",
       "8139     0.133         0.496           53         108        20   \n",
       "8192     0.188         0.507           41          94        13   \n",
       "8223     0.085         0.503           34          72        13   \n",
       "8228     0.073         0.452           31          93        16   \n",
       "8295     0.135         0.629           14          19         6   \n",
       "8298     0.148         0.495           59         134        25   \n",
       "8299     0.130         0.570           50          84        15   \n",
       "8300     0.165         0.469           41         111        13   \n",
       "8301     0.098         0.468           41         101         5   \n",
       "8308     0.016         0.570           36          67         9   \n",
       "8324     0.164         0.533           43          78        14   \n",
       "8326     0.099         0.509           43          84        19   \n",
       "8373     0.082         0.490           43          81        10   \n",
       "8416     0.080         0.612           38          99        13   \n",
       "8419     0.159         0.511           31          61         9   \n",
       "8439     0.136         0.436           39         137        22   \n",
       "8496     0.221         0.561           24          88        18   \n",
       "8519    -0.008         0.530           13          66         9   \n",
       "8533     0.037         0.471           56          89        18   \n",
       "8541     0.075         0.500           27          72         3   \n",
       "8555     0.174         0.577           43          56         8   \n",
       "8583     0.057         0.485           25          76        15   \n",
       "8628     0.129         0.513           64         121        21   \n",
       "8651     0.085         0.523           56         113        25   \n",
       "\n",
       "      personal_pron  possessive_pron  verb  function_word_count  word_count  \\\n",
       "40              200                0    88                  399        1835   \n",
       "73               98                0    59                  235        1249   \n",
       "104             122                0    53                  249        1203   \n",
       "114             126                0    69                  311        1626   \n",
       "123             140                0    85                  266        1288   \n",
       "174             146                0    70                  306        1483   \n",
       "188             138                0    88                  270        1573   \n",
       "236              55                0    31                  108         650   \n",
       "254             124                0    60                  250        1364   \n",
       "303             169                0    68                  342        1777   \n",
       "382               1                0     0                    2           8   \n",
       "431              89                0    46                  189         978   \n",
       "442             148                0    65                  320        1502   \n",
       "463             114                0    58                  245        1265   \n",
       "474             156                0    78                  322        1622   \n",
       "478             189                0   112                  351        1584   \n",
       "509             113                0    70                  240        1195   \n",
       "523             111                0    49                  257        1352   \n",
       "567              56                1    24                  176         912   \n",
       "570             223                2   101                  428        2077   \n",
       "607             186                1   119                  370        1737   \n",
       "621             144                1    88                  337        1643   \n",
       "635              57                0    34                  117         657   \n",
       "638             193                0    94                  359        1722   \n",
       "665              19                0    15                   57         281   \n",
       "670             161                0    84                  285        1483   \n",
       "678             152                0    76                  393        1775   \n",
       "736              17                0    10                   40         212   \n",
       "744              72                0    36                  142         815   \n",
       "751             116                0    64                  279        1545   \n",
       "...             ...              ...   ...                  ...         ...   \n",
       "7904            116                0    68                  271        1531   \n",
       "7939            119                1    79                  328        1786   \n",
       "7945            123                0    58                  211        1015   \n",
       "7973            148                1    90                  321        1444   \n",
       "7995            148                0    90                  308        1616   \n",
       "8040            145                0    80                  281        1301   \n",
       "8139            211                1    91                  393        1939   \n",
       "8192            117                0    56                  265        1333   \n",
       "8223            121                1    57                  241        1214   \n",
       "8228            177                1    78                  318        1421   \n",
       "8295             31                0    18                   70         360   \n",
       "8298            236                0   140                  454        2069   \n",
       "8299            139                1    79                  289        1472   \n",
       "8300             96                0    61                  261        1493   \n",
       "8301             97                2    66                  246        1294   \n",
       "8308             74                0    57                  186        1144   \n",
       "8324            119                1    78                  255        1240   \n",
       "8326            198                0   101                  344        1668   \n",
       "8373            130                0    65                  264        1310   \n",
       "8416            157                0    71                  307        1593   \n",
       "8419            113                0    51                  214         950   \n",
       "8439            104                0    77                  302        1675   \n",
       "8496             95                0    69                  225        1249   \n",
       "8519             73                0    31                  161         857   \n",
       "8533            175                0    79                  338        1642   \n",
       "8541             90                0    35                  192        1024   \n",
       "8555            117                0    53                  224        1155   \n",
       "8583            119                0    78                  235        1184   \n",
       "8628            142                0    96                  348        1877   \n",
       "8651            145                0    93                  339        1543   \n",
       "\n",
       "      num_of_entries  avg_words_post  \n",
       "40                47            39.0  \n",
       "73                46            27.0  \n",
       "104               46            26.0  \n",
       "114               47            34.0  \n",
       "123               44            29.0  \n",
       "174               44            33.0  \n",
       "188               47            33.0  \n",
       "236               32            20.0  \n",
       "254               47            29.0  \n",
       "303               46            38.0  \n",
       "382                1             8.0  \n",
       "431               38            25.0  \n",
       "442               45            33.0  \n",
       "463               44            28.0  \n",
       "474               47            34.0  \n",
       "478               49            32.0  \n",
       "509               46            25.0  \n",
       "523               39            34.0  \n",
       "567               37            24.0  \n",
       "570               50            41.0  \n",
       "607               49            35.0  \n",
       "621               48            34.0  \n",
       "635               33            19.0  \n",
       "638               50            34.0  \n",
       "665               20            14.0  \n",
       "670               48            30.0  \n",
       "678               47            37.0  \n",
       "736                7            30.0  \n",
       "744               33            24.0  \n",
       "751               40            38.0  \n",
       "...              ...             ...  \n",
       "7904              50            30.0  \n",
       "7939              48            37.0  \n",
       "7945              39            26.0  \n",
       "7973              47            30.0  \n",
       "7995              48            33.0  \n",
       "8040              46            28.0  \n",
       "8139              50            38.0  \n",
       "8192              40            33.0  \n",
       "8223              44            27.0  \n",
       "8228              47            30.0  \n",
       "8295              12            30.0  \n",
       "8298              50            41.0  \n",
       "8299              48            30.0  \n",
       "8300              45            33.0  \n",
       "8301              46            28.0  \n",
       "8308              42            27.0  \n",
       "8324              37            33.0  \n",
       "8326              49            34.0  \n",
       "8373              30            43.0  \n",
       "8416              49            32.0  \n",
       "8419              31            30.0  \n",
       "8439              45            37.0  \n",
       "8496              42            29.0  \n",
       "8519              37            23.0  \n",
       "8533              48            34.0  \n",
       "8541              45            22.0  \n",
       "8555              49            23.0  \n",
       "8583              41            28.0  \n",
       "8628              49            38.0  \n",
       "8651              43            35.0  \n",
       "\n",
       "[337 rows x 19 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge_clean_posts.loc[df_merge_clean_posts['ptype_label'] == 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df_merge_clean_posts['ptype_label']\n",
    "X = df_merge_clean_posts.iloc[:,7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.15, random_state=42)\n",
    "\n",
    "\n",
    "scaler=StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2319272254029546"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross validate using random forest\n",
    "rf = RandomForestClassifier(n_estimators=100,max_depth=5)\n",
    "rf.fit(X_train, y_train)\n",
    "pred = rf.predict(X_test)\n",
    "accuracy_score(pred,y_test)\n",
    "cross_validate(rf, X_train,y_train)['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbour best param:  {'metric': 'euclidean', 'n_neighbors': 50, 'weights': 'distance'}\n",
      "K Nearest Neighbour acc score: 22.19 %\n",
      "Logistic Regression best param:  {'C': 1.0, 'penalty': 'l1'}\n",
      "Logistic Regression acc score: 23.74 %\n",
      "Random Forest best param:  {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 200}\n",
      "Random Forest acc score: 23.74 %\n",
      "Gaussian Naive Bayes score: 18.13 %\n",
      "Bernoulli Naive Bayes score: 20.51 %\n"
     ]
    }
   ],
   "source": [
    "# Scoring with different models\n",
    "multiclass_score(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.24423963133640553\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENFJ</th>\n",
       "      <th>ENFP</th>\n",
       "      <th>ENTJ</th>\n",
       "      <th>ENTP</th>\n",
       "      <th>ESFJ</th>\n",
       "      <th>ESFP</th>\n",
       "      <th>ESTJ</th>\n",
       "      <th>ESTP</th>\n",
       "      <th>INFJ</th>\n",
       "      <th>INFP</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>INTP</th>\n",
       "      <th>ISFJ</th>\n",
       "      <th>ISFP</th>\n",
       "      <th>ISTJ</th>\n",
       "      <th>ISTP</th>\n",
       "      <th>total</th>\n",
       "      <th>correct</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENFJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENFP</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTP</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESFJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESFP</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESTJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESTP</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFJ</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>141</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>221</td>\n",
       "      <td>41</td>\n",
       "      <td>0.185520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFP</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>185</td>\n",
       "      <td>7</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>275</td>\n",
       "      <td>185</td>\n",
       "      <td>0.672727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>78</td>\n",
       "      <td>8</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>164</td>\n",
       "      <td>8</td>\n",
       "      <td>0.048780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTP</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>77</td>\n",
       "      <td>6</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>196</td>\n",
       "      <td>83</td>\n",
       "      <td>0.423469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISFJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISFP</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISTJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISTP</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ENFJ  ENFP  ENTJ  ENTP  ESFJ  ESFP  ESTJ  ESTP  INFJ  INFP  INTJ  INTP  \\\n",
       "ENFJ     0     0     0     0     0     0     0     0     9    18     1     0   \n",
       "ENFP     0     1     0     0     0     0     0     0    18    65     2    15   \n",
       "ENTJ     0     0     0     0     0     0     0     0     3    19     1    12   \n",
       "ENTP     0     0     0     0     0     0     0     0    13    51     2    37   \n",
       "ESFJ     0     0     0     0     0     0     0     0     2     4     0     0   \n",
       "ESFP     0     0     0     0     0     0     0     0     0     3     0     4   \n",
       "ESTJ     0     0     0     0     0     0     0     0     1     5     0     0   \n",
       "ESTP     0     0     0     0     0     0     0     0     1     6     0     6   \n",
       "INFJ     0     1     0     0     0     0     0     0    41   141     7    31   \n",
       "INFP     0     0     0     0     0     0     0     0    47   185     7    36   \n",
       "INTJ     0     0     0     0     0     0     0     0    19    78     8    59   \n",
       "INTP     0     0     0     1     0     0     0     0    29    77     6    83   \n",
       "ISFJ     0     0     0     0     0     0     0     0     5    17     0     3   \n",
       "ISFP     0     0     0     0     0     0     0     0     5    30     0     6   \n",
       "ISTJ     0     0     0     0     0     0     0     0     4    15     1    11   \n",
       "ISTP     0     0     0     0     0     0     0     0     5    21     2    22   \n",
       "\n",
       "      ISFJ  ISFP  ISTJ  ISTP  total  correct  accuracy  \n",
       "ENFJ     0     0     0     0     28        0  0.000000  \n",
       "ENFP     0     0     0     0    101        1  0.009901  \n",
       "ENTJ     0     0     0     0     35        0  0.000000  \n",
       "ENTP     0     0     0     0    103        0  0.000000  \n",
       "ESFJ     0     0     0     0      6        0  0.000000  \n",
       "ESFP     0     0     0     0      7        0  0.000000  \n",
       "ESTJ     0     0     0     0      6        0  0.000000  \n",
       "ESTP     0     0     0     0     13        0  0.000000  \n",
       "INFJ     0     0     0     0    221       41  0.185520  \n",
       "INFP     0     0     0     0    275      185  0.672727  \n",
       "INTJ     0     0     0     0    164        8  0.048780  \n",
       "INTP     0     0     0     0    196       83  0.423469  \n",
       "ISFJ     0     0     0     0     25        0  0.000000  \n",
       "ISFP     0     0     0     0     41        0  0.000000  \n",
       "ISTJ     0     0     0     0     31        0  0.000000  \n",
       "ISTP     0     0     0     0     50        0  0.000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_df(LogisticRegression(),X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TFIDF Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df_merge_tfidf['ptype_label']\n",
    "X = df_merge_tfidf.iloc[:,7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# multiclass_score(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.23348694316436253\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENFJ</th>\n",
       "      <th>ENFP</th>\n",
       "      <th>ENTJ</th>\n",
       "      <th>ENTP</th>\n",
       "      <th>ESFJ</th>\n",
       "      <th>ESFP</th>\n",
       "      <th>ESTJ</th>\n",
       "      <th>ESTP</th>\n",
       "      <th>INFJ</th>\n",
       "      <th>INFP</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>INTP</th>\n",
       "      <th>ISFJ</th>\n",
       "      <th>ISFP</th>\n",
       "      <th>ISTJ</th>\n",
       "      <th>ISTP</th>\n",
       "      <th>total</th>\n",
       "      <th>correct</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENFJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENFP</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTP</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESFJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESFP</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESTJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESTP</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>163</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>221</td>\n",
       "      <td>25</td>\n",
       "      <td>0.113122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFP</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>198</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>275</td>\n",
       "      <td>198</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>86</td>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>164</td>\n",
       "      <td>5</td>\n",
       "      <td>0.030488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTP</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>89</td>\n",
       "      <td>5</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>196</td>\n",
       "      <td>76</td>\n",
       "      <td>0.387755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISFJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISFP</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISTJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISTP</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ENFJ  ENFP  ENTJ  ENTP  ESFJ  ESFP  ESTJ  ESTP  INFJ  INFP  INTJ  INTP  \\\n",
       "ENFJ     0     0     0     0     0     0     0     0     6    21     1     0   \n",
       "ENFP     0     0     0     0     0     0     0     0    11    75     0    15   \n",
       "ENTJ     0     0     0     0     0     0     0     0     2    21     1    11   \n",
       "ENTP     0     0     0     0     0     0     0     0    10    54     1    38   \n",
       "ESFJ     0     0     0     0     0     0     0     0     2     4     0     0   \n",
       "ESFP     0     0     0     0     0     0     0     0     0     4     0     3   \n",
       "ESTJ     0     0     0     0     0     0     0     0     1     5     0     0   \n",
       "ESTP     0     0     0     0     0     0     0     0     1     6     0     6   \n",
       "INFJ     0     0     0     0     0     0     0     0    25   163     3    30   \n",
       "INFP     0     0     0     0     0     0     0     0    38   198     3    36   \n",
       "INTJ     0     0     0     0     0     0     0     0    19    86     5    54   \n",
       "INTP     0     0     0     0     0     0     0     0    26    89     5    76   \n",
       "ISFJ     0     0     0     0     0     0     0     0     1    23     0     1   \n",
       "ISFP     0     0     0     0     0     0     0     0     3    33     0     5   \n",
       "ISTJ     0     0     0     0     0     0     0     0     5    17     1     8   \n",
       "ISTP     0     0     0     0     0     0     0     0     4    27     1    18   \n",
       "\n",
       "      ISFJ  ISFP  ISTJ  ISTP  total  correct  accuracy  \n",
       "ENFJ     0     0     0     0     28        0  0.000000  \n",
       "ENFP     0     0     0     0    101        0  0.000000  \n",
       "ENTJ     0     0     0     0     35        0  0.000000  \n",
       "ENTP     0     0     0     0    103        0  0.000000  \n",
       "ESFJ     0     0     0     0      6        0  0.000000  \n",
       "ESFP     0     0     0     0      7        0  0.000000  \n",
       "ESTJ     0     0     0     0      6        0  0.000000  \n",
       "ESTP     0     0     0     0     13        0  0.000000  \n",
       "INFJ     0     0     0     0    221       25  0.113122  \n",
       "INFP     0     0     0     0    275      198  0.720000  \n",
       "INTJ     0     0     0     0    164        5  0.030488  \n",
       "INTP     0     0     0     0    196       76  0.387755  \n",
       "ISFJ     0     0     0     0     25        0  0.000000  \n",
       "ISFP     0     0     0     0     41        0  0.000000  \n",
       "ISTJ     0     0     0     0     31        0  0.000000  \n",
       "ISTP     0     0     0     0     50        0  0.000000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_df(LogisticRegression(),X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Oversampling clean posts\n",
    "Here We'll see if the score can be improved with oversampling the train set to get a better prediction on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df_merge_clean_posts['ptype_label']\n",
    "X = df_merge_clean_posts.iloc[:,7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7373, 12)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train test split on the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.15, random_state=42)\n",
    "\n",
    "scaler=StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross Validate\n",
    "To cross validate the train set with oversampling function, the following are the steps applied\n",
    "1. Stratify and split the train set into 5 folds. \n",
    "2. Loop through each fold. Within each fold, the trainX and trainY is oversampled.\n",
    "3. Train the model with the oversampled set.\n",
    "4. Predict the testX \n",
    "5. Score it against testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train,) \n",
    "y_train = pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape:  5890\n",
      "Resampled dataset shape:  19920\n",
      "K Nearest Neighbour training accuracy score:  0.1534804753820034\n",
      "K Nearest Neighbour accuracy score:  0.041132838840188805\n",
      "Logistic Regression training accuracy score:  0.08455008488964347\n",
      "Logistic Regression accuracy score:  0.0815913688469319\n",
      "Random forest training accuracy score:  0.6813242784380306\n",
      "Random forest accuracy score:  0.10451786918408631\n",
      "****************************************************************************************************\n",
      "Original dataset shape:  5895\n",
      "Resampled dataset shape:  19920\n",
      "K Nearest Neighbour training accuracy score:  0.14995759117896523\n",
      "K Nearest Neighbour accuracy score:  0.044654939106901215\n",
      "Logistic Regression training accuracy score:  0.09245122985581\n",
      "Logistic Regression accuracy score:  0.07780784844384303\n",
      "Random forest training accuracy score:  0.6703986429177269\n",
      "Random forest accuracy score:  0.09878213802435724\n",
      "****************************************************************************************************\n",
      "Original dataset shape:  5900\n",
      "Resampled dataset shape:  19936\n",
      "K Nearest Neighbour training accuracy score:  0.15440677966101696\n",
      "K Nearest Neighbour accuracy score:  0.04276985743380855\n",
      "Logistic Regression training accuracy score:  0.0776271186440678\n",
      "Logistic Regression accuracy score:  0.07603530210454854\n",
      "Random forest training accuracy score:  0.6593220338983051\n",
      "Random forest accuracy score:  0.09911744738628649\n",
      "****************************************************************************************************\n",
      "Original dataset shape:  5902\n",
      "Resampled dataset shape:  19936\n",
      "K Nearest Neighbour training accuracy score:  0.1513046424940698\n",
      "K Nearest Neighbour accuracy score:  0.04758667573079538\n",
      "Logistic Regression training accuracy score:  0.08844459505252457\n",
      "Logistic Regression accuracy score:  0.08157715839564922\n",
      "Random forest training accuracy score:  0.6782446628261606\n",
      "Random forest accuracy score:  0.11624745071380013\n",
      "****************************************************************************************************\n",
      "Original dataset shape:  5905\n",
      "Resampled dataset shape:  19936\n",
      "K Nearest Neighbour training accuracy score:  0.16257408975444537\n",
      "K Nearest Neighbour accuracy score:  0.04495912806539509\n",
      "Logistic Regression training accuracy score:  0.07891617273497037\n",
      "Logistic Regression accuracy score:  0.0681198910081744\n",
      "Random forest training accuracy score:  0.6104995766299746\n",
      "Random forest accuracy score:  0.0946866485013624\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Train test split on train sets\n",
    "\n",
    "X_train = pd.DataFrame(X_train) \n",
    "y_train = pd.DataFrame(y_train)\n",
    "\n",
    "kf = StratifiedKFold(5, random_state=42)\n",
    "\n",
    "for trainIdx, evalIdx in kf.split(X_train,y_train):\n",
    "    \n",
    "    trainX, testX = X_train.iloc[trainIdx], X_train.iloc[evalIdx]\n",
    "    trainY, testY = y_train.iloc[trainIdx], y_train.iloc[evalIdx]\n",
    "    \n",
    "    #Original data size\n",
    "    print('Original dataset shape: ',len(trainX))                      \n",
    "    \n",
    "    #resampling trainX and trainY set requires dataframe\n",
    "    sm = SMOTE()                                        \n",
    "    trainX_res, trainY_res = sm.fit_sample(trainX,trainY)\n",
    "    \n",
    "    #resampled data size\n",
    "    print('Resampled dataset shape: ',len(trainX_res))\n",
    "\n",
    "    \n",
    "    # fit into model\n",
    "    \n",
    "    classifier = KNeighborsClassifier(n_neighbors=50).fit(trainX_res,trainY_res)\n",
    "    y_pred = classifier.predict(trainX)\n",
    "    print('K Nearest Neighbour training accuracy score: ',accuracy_score(y_true=trainY, y_pred = y_pred))\n",
    "    y_pred = classifier.predict(testX)\n",
    "    print('K Nearest Neighbour accuracy score: ',accuracy_score(y_true=testY, y_pred = y_pred))\n",
    "\n",
    "    \n",
    "    classifier = LogisticRegression().fit(trainX_res,trainY_res)\n",
    "    y_pred = classifier.predict(trainX)\n",
    "    print('Logistic Regression training accuracy score: ',accuracy_score(y_true=trainY, y_pred = y_pred))\n",
    "    y_pred = classifier.predict(testX)\n",
    "    print('Logistic Regression accuracy score: ',accuracy_score(y_true=testY, y_pred = y_pred))\n",
    "\n",
    "    classifier = RandomForestClassifier(n_estimators=200,max_depth=10).fit(trainX_res,trainY_res)\n",
    "    y_pred = classifier.predict(trainX)\n",
    "    print('Random forest training accuracy score: ',accuracy_score(y_true=trainY, y_pred = y_pred))\n",
    "    y_pred = classifier.predict(testX)\n",
    "    print('Random forest accuracy score: ',accuracy_score(y_true=testY, y_pred = y_pred))\n",
    "\n",
    "\n",
    "    print('*'*100)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparison between imbalanced and oversampled data\n",
    "Compared to the imbalanced set, thought the accuracy reduced, we see that \n",
    "- the model distributes a much balanced weight to all types. The lowest count fot bottom 2 types are not captured in the prediction. \n",
    "- The model is less bias.\n",
    "- It is also evident from the accuracy score differences across the 16 types are much lesser apart from one another "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1468, 1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.0994550408719346\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENFJ</th>\n",
       "      <th>ENFP</th>\n",
       "      <th>ENTJ</th>\n",
       "      <th>ENTP</th>\n",
       "      <th>ESFJ</th>\n",
       "      <th>ESFP</th>\n",
       "      <th>ESTJ</th>\n",
       "      <th>ESTP</th>\n",
       "      <th>INFJ</th>\n",
       "      <th>INFP</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>INTP</th>\n",
       "      <th>ISFJ</th>\n",
       "      <th>ISFP</th>\n",
       "      <th>ISTJ</th>\n",
       "      <th>ISTP</th>\n",
       "      <th>total</th>\n",
       "      <th>correct</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENFJ</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENFP</th>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>114</td>\n",
       "      <td>16</td>\n",
       "      <td>0.140351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTJ</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENTP</th>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>116</td>\n",
       "      <td>14</td>\n",
       "      <td>0.120690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESFJ</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESFP</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESTJ</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESTP</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFJ</th>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>249</td>\n",
       "      <td>18</td>\n",
       "      <td>0.072289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INFP</th>\n",
       "      <td>22</td>\n",
       "      <td>39</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>26</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>311</td>\n",
       "      <td>28</td>\n",
       "      <td>0.090032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>185</td>\n",
       "      <td>28</td>\n",
       "      <td>0.151351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTP</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>35</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>221</td>\n",
       "      <td>22</td>\n",
       "      <td>0.099548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISFJ</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISFP</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>0.021739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISTJ</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>0.029412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISTP</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>57</td>\n",
       "      <td>7</td>\n",
       "      <td>0.122807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ENFJ  ENFP  ENTJ  ENTP  ESFJ  ESFP  ESTJ  ESTP  INFJ  INFP  INTJ  INTP  \\\n",
       "ENFJ     3     5     2     1     1     1     3     5     1     4     1     1   \n",
       "ENFP    21    16     5     1     6     6     7     9     9     8     1     1   \n",
       "ENTJ     0     3     3     2     2     1     3     3     4     2     3     4   \n",
       "ENTP     6    14     8    14     6     4     4    10     8     5     4     7   \n",
       "ESFJ     0     1     3     0     0     0     1     0     1     0     0     0   \n",
       "ESFP     1     1     0     2     1     1     1     0     0     0     1     0   \n",
       "ESTJ     1     1     0     0     1     1     0     0     0     0     0     1   \n",
       "ESTP     0     1     1     1     0     1     0     2     1     1     0     1   \n",
       "INFJ    20    16    12    12    14    17     9    21    18    21    14    18   \n",
       "INFP    22    39    17     9    16    15    16    14    26    28    17    17   \n",
       "INTJ     7    12    18    11     4     9    13     9     4     1    28    19   \n",
       "INTP     6    10    22    16     3    15     7    13    13     9    35    22   \n",
       "ISFJ     3     2     3     1     1     1     0     2     2     3     1     1   \n",
       "ISFP     4     5     4     3     4     3     1     1     7     3     0     2   \n",
       "ISTJ     2     4     3     3     2     1     1     3     4     1     1     1   \n",
       "ISTP     0     2     5     3     5     0     4     3     3     4     5     7   \n",
       "\n",
       "      ISFJ  ISFP  ISTJ  ISTP  total  correct  accuracy  \n",
       "ENFJ     1     1     2     0     32        3  0.093750  \n",
       "ENFP     6     5     6     7    114       16  0.140351  \n",
       "ENTJ     2     2     2     3     39        3  0.076923  \n",
       "ENTP     6     4     8     8    116       14  0.120690  \n",
       "ESFJ     1     0     0     0      7        0  0.000000  \n",
       "ESFP     0     0     0     0      8        1  0.125000  \n",
       "ESTJ     0     0     1     0      6        0  0.000000  \n",
       "ESTP     2     2     1     1     15        2  0.133333  \n",
       "INFJ    12    10    21    14    249       18  0.072289  \n",
       "INFP    16    20    17    22    311       28  0.090032  \n",
       "INTJ     9    13    19     9    185       28  0.151351  \n",
       "INTP     6    11    18    15    221       22  0.099548  \n",
       "ISFJ     2     3     2     1     28        2  0.071429  \n",
       "ISFP     2     1     1     5     46        1  0.021739  \n",
       "ISTJ     1     2     1     4     34        1  0.029412  \n",
       "ISTP     1     6     2     7     57        7  0.122807  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix to see why the score was lower    \n",
    "cm_df(RandomForestClassifier(n_estimators=200,max_depth=10),\n",
    "     X_train=trainX_res,\n",
    "     y_train=trainY_res,\n",
    "     X_test=testX,\n",
    "     y_test=testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scoirng with classifier\n",
    "\n",
    "Surprisingly with a balanced dataset, Logisitc Regression score badly. K nearest neighbour scored the highest  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling with Type Dichotomy as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pchar</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>world_I</th>\n",
       "      <td>6676</td>\n",
       "      <td>0.769568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world_E</th>\n",
       "      <td>1999</td>\n",
       "      <td>0.230432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>information_N</th>\n",
       "      <td>7478</td>\n",
       "      <td>0.862017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>information_S</th>\n",
       "      <td>1197</td>\n",
       "      <td>0.137983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision_F</th>\n",
       "      <td>4694</td>\n",
       "      <td>0.541095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision_T</th>\n",
       "      <td>3981</td>\n",
       "      <td>0.458905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>structure_P</th>\n",
       "      <td>5241</td>\n",
       "      <td>0.604150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>structure_J</th>\n",
       "      <td>3434</td>\n",
       "      <td>0.395850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               pchar  percentage\n",
       "world_I         6676    0.769568\n",
       "world_E         1999    0.230432\n",
       "information_N   7478    0.862017\n",
       "information_S   1197    0.137983\n",
       "decision_F      4694    0.541095\n",
       "decision_T      3981    0.458905\n",
       "structure_P     5241    0.604150\n",
       "structure_J     3434    0.395850"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pchar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_score(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test):\n",
    "\n",
    "#     print('K-Nearest Neighbour Classifer')\n",
    "#     param_grid = dict(n_neighbors=[5], \n",
    "#                       weights=['distance'], \n",
    "#                       metric=['euclidean'])\n",
    "    \n",
    "#     grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3, )\n",
    "#     grid.fit(X_train, y_train)\n",
    "#     y_pred = grid.best_estimator_.predict(X_test)\n",
    "#     print('Best params: ', grid.best_params_)\n",
    "#     print('recall score: {0:.2f}'.format(recall_score(y_true = y_test, y_pred=y_pred)))\n",
    "#     print('precision score: {0:.2f}'.format(precision_score(y_true = y_test, y_pred=y_pred)))\n",
    "#     print('*'*100)\n",
    "    \n",
    "#     print('Random Forest Classifer')\n",
    "#     param_grid = dict(n_estimators= [10,50],\n",
    "#                           max_depth = [5,10],\n",
    "#                           max_features= ['auto', 'sqrt', 'log2'])\n",
    "#     grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=3,)\n",
    "#     grid.fit(X_train, y_train)\n",
    "#     y_pred = grid.best_estimator_.predict(X_test)\n",
    "#     print('Best params: ', grid.best_params_)\n",
    "#     print('recall score: {0:.2f}'.format(recall_score(y_true = y_test, y_pred=y_pred)))\n",
    "#     print('precision score: {0:.2f}'.format(precision_score(y_true = y_test, y_pred=y_pred)))\n",
    "#     print('F1 score: {0:.2f}'.format(f1_score(y_true = y_test, y_pred=y_pred)))\n",
    "#     print('*'*100)\n",
    "    \n",
    "    \n",
    "    print('Logistic Regression Classifer')\n",
    "    param_grid = dict(penalty =['l1','l2'], \n",
    "                      C= np.logspace(0, 4, 10))\n",
    "                      \n",
    "    grid = GridSearchCV(LogisticRegression(), param_grid, cv=3, scoring='accuracy',return_train_score=True)\n",
    "    grid.fit(X_train, y_train)\n",
    "    y_pred = grid.best_estimator_.predict(X_test)\n",
    "    print('Best params: ', grid.best_params_)\n",
    "    print('recall score: {0:.2f}'.format(recall_score(y_true = y_test, y_pred=y_pred)))\n",
    "    print('precision score: {0:.2f}'.format(precision_score(y_true = y_test, y_pred=y_pred)))\n",
    "    print('F1 score: {0:.2f}'.format(f1_score(y_true = y_test, y_pred=y_pred)))\n",
    "\n",
    "    print('*'*100)\n",
    "\n",
    "    \n",
    "#     print('Support Vector Machine Classifier')\n",
    "    \n",
    "#     param_grid = dict(C = [ 0.1, 1, 10],\n",
    "#                       gamma = [ 0.1, 1],\n",
    "#                       kernel=['rbf','poly'])\n",
    "                      \n",
    "#     grid = svm.SVC()\n",
    "#     grid.fit(X_train, y_train)\n",
    "#     y_pred = grid.predict(X_test)\n",
    "#     print('Best params: ', grid.best_params_)\n",
    "#     print('recall score: {0:.2f}'.format(recall_score(y_true = y_test, y_pred=y_pred)))\n",
    "#     print('precision score: {0:.2f}'.format(precision_score(y_true = y_test, y_pred=y_pred)))\n",
    "#     print('*'*100)\n",
    "    \n",
    "    print('Naive Bayes Gaussian Classifier')\n",
    "\n",
    "    gb = nb.GaussianNB()\n",
    "    gb.fit(X_train, y_train)\n",
    "    y_pred = gb.predict(X_test)\n",
    "    print('recall score: {0:.2f}'.format(recall_score(y_true = y_test, y_pred=y_pred)))\n",
    "    print('precision score: {0:.2f}'.format(precision_score(y_true = y_test, y_pred=y_pred)))\n",
    "    print('F1 score: {0:.2f}'.format(f1_score(y_true = y_test, y_pred=y_pred)))\n",
    "\n",
    "    print('*'*100) \n",
    "    \n",
    "#     print('Naive Bayes Bernoulli Classifer')\n",
    "\n",
    "#     bn = nb.BernoulliNB()\n",
    "#     bn.fit(X_train,y_train)\n",
    "#     pred_y = bn.predict(X_test)\n",
    "#     print('recall score: {0:.2f}'.format(recall_score(y_true = y_test, y_pred=y_pred)))\n",
    "#     print('precision score: {0:.2f}'.format(precision_score(y_true = y_test, y_pred=y_pred)))\n",
    "#     print('F1 score: {0:.2f}'.format(f1_score(y_true = y_test, y_pred=y_pred)))\n",
    "\n",
    "#     print('*'*100) \n",
    "    \n",
    "#     print('Naive Bayes MultiNomial Classifer')\n",
    "\n",
    "#     bn = nb.MultinomialNB()\n",
    "#     bn.fit(X_train,y_train)\n",
    "#     pred_y = bn.predict(X_test)\n",
    "#     print('recall score: {0:.2f}'.format(recall_score(y_true = y_test, y_pred=y_pred)))\n",
    "#     print('precision score: {0:.2f}'.format(precision_score(y_true = y_test, y_pred=y_pred)))\n",
    "#     print('F1 score: {0:.2f}'.format(f1_score(y_true = y_test, y_pred=y_pred)))\n",
    "#     print('*'*100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifer\n",
      "Best params:  {'C': 1.0, 'penalty': 'l1'}\n",
      "recall score: 0.17\n",
      "precision score: 0.71\n",
      "F1 score: 0.27\n",
      "****************************************************************************************************\n",
      "Naive Bayes Gaussian Classifier\n",
      "recall score: 0.61\n",
      "precision score: 0.38\n",
      "F1 score: 0.46\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "#World E&I\n",
    "y = df_merge_tfidf.world_E_I\n",
    "X = df_merge_tfidf.iloc[:,6:]\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42,stratify=y)\n",
    "\n",
    "binary_score(X_train=X_train,\n",
    "             y_train=y_train,\n",
    "             X_test=X_test,\n",
    "             y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifer\n",
      "Best params:  {'C': 1.0, 'penalty': 'l2'}\n",
      "recall score: 0.01\n",
      "precision score: 0.50\n",
      "F1 score: 0.01\n",
      "****************************************************************************************************\n",
      "Naive Bayes Gaussian Classifier\n",
      "recall score: 0.57\n",
      "precision score: 0.26\n",
      "F1 score: 0.35\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "#Information S&N\n",
    "y = df_merge_tfidf.information_S_N\n",
    "X = df_merge_tfidf.iloc[:,6:]\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42,stratify=y)\n",
    "\n",
    "binary_score(X_train=X_train,\n",
    "             y_train=y_train,\n",
    "             X_test=X_test,\n",
    "             y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifer\n",
      "Best params:  {'C': 1.0, 'penalty': 'l2'}\n",
      "recall score: 0.75\n",
      "precision score: 0.79\n",
      "F1 score: 0.77\n",
      "****************************************************************************************************\n",
      "Naive Bayes Gaussian Classifier\n",
      "recall score: 0.73\n",
      "precision score: 0.71\n",
      "F1 score: 0.72\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "#Decision T&F\n",
    "y = df_merge_tfidf.decision_T_F\n",
    "X = df_merge_tfidf.iloc[:,6:]\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42,stratify=y)\n",
    "\n",
    "binary_score(X_train=X_train,\n",
    "             y_train=y_train,\n",
    "             X_test=X_test,\n",
    "             y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifer\n",
      "Best params:  {'C': 1.0, 'penalty': 'l1'}\n",
      "recall score: 0.40\n",
      "precision score: 0.64\n",
      "F1 score: 0.49\n",
      "****************************************************************************************************\n",
      "Naive Bayes Gaussian Classifier\n",
      "recall score: 0.56\n",
      "precision score: 0.49\n",
      "F1 score: 0.52\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "#Structure J&P\n",
    "y = df_merge_tfidf.structure_J_P\n",
    "X = df_merge_tfidf.iloc[:,6:]\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42,stratify=y)\n",
    "\n",
    "binary_score(X_train=X_train,\n",
    "             y_train=y_train,\n",
    "             X_test=X_test,\n",
    "             y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes Gaussian\n",
    "The process took too long and naive Bayes scored the highest.    \n",
    "Here we are going to tweak the model further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#World E&I\n",
    "y = df_merge_tfidf.world_E_I\n",
    "X = df_merge_tfidf.iloc[:,6:]\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42,stratify=y)\n",
    "\n",
    "# initalize\n",
    "sm=SMOTE()\n",
    "X_res, y_res = sm.fit_sample(X_train,y_train)\n",
    "gb = LogisticRegression()\n",
    "gb.fit(X_res, y_res)\n",
    "\n",
    "# predict\n",
    "EI_pred = gb.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# information_S_N\n",
    "y = df_merge_tfidf.information_S_N\n",
    "X = df_merge_tfidf.iloc[:,6:]\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42,stratify=y)\n",
    "\n",
    "# initalize\n",
    "sm=SMOTE()\n",
    "X_res, y_res = sm.fit_sample(X_train,y_train)\n",
    "gb = LogisticRegression()\n",
    "gb.fit(X_res, y_res)\n",
    "\n",
    "# predict\n",
    "SN_pred=gb.predict_proba(X_test)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision_T_F\n",
    "y = df_merge_tfidf.decision_T_F\n",
    "X = df_merge_tfidf.iloc[:,6:]\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42,stratify=y)\n",
    "\n",
    "# initalize\n",
    "sm=SMOTE()\n",
    "X_res, y_res = sm.fit_sample(X_train,y_train)\n",
    "gb = LogisticRegression()\n",
    "gb.fit(X_res, y_res)\n",
    "\n",
    "# predict\n",
    "TF_pred=gb.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure_J_P\n",
    "y = df_merge_tfidf.structure_J_P\n",
    "X = df_merge_tfidf.iloc[:,6:]\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42,stratify=y)\n",
    "\n",
    "# initalize\n",
    "sm=SMOTE()\n",
    "X_res, y_res = sm.fit_sample(X_train,y_train)\n",
    "gb = LogisticRegression()\n",
    "gb.fit(X_res, y_res)\n",
    "\n",
    "# predict\n",
    "JP_pred=gb.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame({'EI':EI_pred,'SN':SN_pred,'TF':TF_pred,'JP':JP_pred})\n",
    "final_test = df_merge_clean_posts.iloc[y_test.index,0:6].reset_index(drop='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusting threshold \n",
    "prediction.EI = prediction.EI.map(lambda x:1 if x>.5 else 0)\n",
    "prediction.SN = prediction.SN.map(lambda x:1 if x>.5 else 0)\n",
    "prediction.TF = prediction.TF.map(lambda x:1 if x>.5 else 0)\n",
    "prediction.JP = prediction.JP.map(lambda x:1 if x>.5 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### compare results after over sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world E&I\n",
      "accuracy score:  0.6\n",
      "recall score:  0.2\n",
      "precision score:  0.28\n",
      "f1 score:  0.24\n",
      "****************************************************************************************************\n",
      "information S&N\n",
      "accuracy score:  0.7\n",
      "recall score:  0.13\n",
      "precision score:  0.25\n",
      "f1 score:  0.17\n",
      "****************************************************************************************************\n",
      "Decision T&F\n",
      "accuracy score:  0.5\n",
      "recall score:  0.45\n",
      "precision score:  0.47\n",
      "f1 score:  0.46\n",
      "****************************************************************************************************\n",
      "Structure J&P\n",
      "accuracy score:  0.66\n",
      "recall score:  0.57\n",
      "precision score:  0.61\n",
      "f1 score:  0.59\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[706, 203],\n",
       "       [313,  80]], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score metric\n",
    "print('world E&I')\n",
    "print('accuracy score: ',round(accuracy_score(prediction.EI,final_test.world_E_I),2))\n",
    "print('recall score: ',round(recall_score(prediction.EI,final_test.world_E_I),2))\n",
    "print('precision score: ',round(precision_score(prediction.EI,final_test.world_E_I),2))\n",
    "print('f1 score: ',round(f1_score(prediction.EI,final_test.world_E_I),2))\n",
    "print('*'*100)\n",
    "\n",
    "print('information S&N')\n",
    "print('accuracy score: ',round(accuracy_score(prediction.SN,final_test.information_S_N),2))\n",
    "print('recall score: ',round(recall_score(prediction.SN,final_test.information_S_N),2))\n",
    "print('precision score: ',round(precision_score(prediction.SN,final_test.information_S_N),2))\n",
    "print('f1 score: ',round(f1_score(prediction.SN,final_test.information_S_N),2))\n",
    "print('*'*100)\n",
    "\n",
    "print('Decision T&F')\n",
    "print('accuracy score: ',round(accuracy_score(prediction.TF,final_test.decision_T_F),2))\n",
    "print('recall score: ',round(recall_score(prediction.TF,final_test.decision_T_F),2))\n",
    "print('precision score: ',round(precision_score(prediction.TF,final_test.decision_T_F),2))\n",
    "print('f1 score: ',round(f1_score(prediction.TF,final_test.decision_T_F),2))\n",
    "print('*'*100)\n",
    "\n",
    "print('Structure J&P')\n",
    "print('accuracy score: ',round(accuracy_score(prediction.JP,final_test.structure_J_P),2))\n",
    "print('recall score: ',round(recall_score(prediction.JP,final_test.structure_J_P),2))\n",
    "print('precision score: ',round(precision_score(prediction.JP,final_test.structure_J_P),2))\n",
    "print('f1 score: ',round(f1_score(prediction.JP,final_test.structure_J_P),2))\n",
    "print('*'*100)\n",
    "\n",
    "confusion_matrix(prediction.EI,final_test.world_E_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction['correct']  =np.zeros(len(prediction.EI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total accuracy\n",
    "res = 0\n",
    "for i in range(len(prediction)): \n",
    "    if (prediction.EI[i] == final_test.world_E_I[i] and \n",
    "        prediction.SN[i] == final_test.information_S_N[i] and\n",
    "        prediction.TF[i] == final_test.decision_T_F[i] and\n",
    "        prediction.JP[i] == final_test.structure_J_P[i]):\n",
    "        res += 1 \n",
    "        prediction.correct[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1382488479262673"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res/len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vocabularies count\n",
    "We find the top vocabularies for each type dichotomies    \n",
    "Then we find how often they appear in their entries\n",
    "\n",
    "I=0\n",
    "E=1\n",
    "\n",
    "N=0\n",
    "S=1\n",
    "\n",
    "F=0\n",
    "T=1\n",
    "\n",
    "P=0\n",
    "J=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>16248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just</th>\n",
       "      <td>13110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>11722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>10600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>8632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count\n",
       "like    16248\n",
       "just    13110\n",
       "think   11722\n",
       "people  10600\n",
       "know     8632"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 50 # number of vocabs\n",
    "\n",
    "top_vocabs_world_I = pd.DataFrame(df_merge_cvn[df_merge_cvn.world_E_I == 0].iloc[:,5:].sum().sort_values(ascending=False).head(n),columns=['count'])\n",
    "top_vocabs_world_E = pd.DataFrame(df_merge_cvn[df_merge_cvn.world_E_I == 1].iloc[:,5:].sum().sort_values(ascending=False).head(n),columns=['count'])\n",
    "\n",
    "top_vocabs_information_N = pd.DataFrame(df_merge_cvn[df_merge_cvn.information_S_N == 0].iloc[:,5:].sum().sort_values(ascending=False).head(n),columns=['count'])\n",
    "top_vocabs_information_S = pd.DataFrame(df_merge_cvn[df_merge_cvn.information_S_N == 1].iloc[:,5:].sum().sort_values(ascending=False).head(n),columns=['count'])\n",
    "\n",
    "top_vocabs_decision_F = pd.DataFrame(df_merge_cvn[df_merge_cvn.decision_T_F == 0].iloc[:,5:].sum().sort_values(ascending=False).head(n),columns=['count'])\n",
    "top_vocabs_decision_T = pd.DataFrame(df_merge_cvn[df_merge_cvn.decision_T_F == 1].iloc[:,5:].sum().sort_values(ascending=False).head(n),columns=['count'])\n",
    "\n",
    "top_vocabs_structure_P = pd.DataFrame(df_merge_cvn[df_merge_cvn.structure_J_P == 0].iloc[:,5:].sum().sort_values(ascending=False).head(n),columns=['count'])\n",
    "top_vocabs_structure_J = pd.DataFrame(df_merge_cvn[df_merge_cvn.structure_J_P == 1].iloc[:,5:].sum().sort_values(ascending=False).head(n),columns=['count'])\n",
    "\n",
    "\n",
    "top_vocabs_world_E.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top vocabs \n",
    "df_top_vocabs = pd.concat([top_vocabs_world_I, top_vocabs_world_E, \n",
    "          top_vocabs_information_N, top_vocabs_information_S,\n",
    "          top_vocabs_decision_F, top_vocabs_decision_T,\n",
    "          top_vocabs_structure_P, top_vocabs_structure_J ],\n",
    "          axis=1,join='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### top vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>e</th>\n",
       "      <th>n</th>\n",
       "      <th>s</th>\n",
       "      <th>f</th>\n",
       "      <th>t</th>\n",
       "      <th>p</th>\n",
       "      <th>j</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actually</th>\n",
       "      <td>9998.0</td>\n",
       "      <td>3127.0</td>\n",
       "      <td>11471.0</td>\n",
       "      <td>1654.0</td>\n",
       "      <td>7148.0</td>\n",
       "      <td>5977.0</td>\n",
       "      <td>8137.0</td>\n",
       "      <td>4988.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>6955.0</td>\n",
       "      <td>2322.0</td>\n",
       "      <td>7977.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>5312.0</td>\n",
       "      <td>3965.0</td>\n",
       "      <td>5571.0</td>\n",
       "      <td>3706.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>better</th>\n",
       "      <td>6388.0</td>\n",
       "      <td>1888.0</td>\n",
       "      <td>7169.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4580.0</td>\n",
       "      <td>3696.0</td>\n",
       "      <td>4909.0</td>\n",
       "      <td>3367.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4567.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>did</th>\n",
       "      <td>14532.0</td>\n",
       "      <td>4328.0</td>\n",
       "      <td>16222.0</td>\n",
       "      <td>2638.0</td>\n",
       "      <td>10042.0</td>\n",
       "      <td>8818.0</td>\n",
       "      <td>11189.0</td>\n",
       "      <td>7671.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>different</th>\n",
       "      <td>6211.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7016.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4835.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>does</th>\n",
       "      <td>13277.0</td>\n",
       "      <td>3942.0</td>\n",
       "      <td>14897.0</td>\n",
       "      <td>2322.0</td>\n",
       "      <td>8817.0</td>\n",
       "      <td>8402.0</td>\n",
       "      <td>10062.0</td>\n",
       "      <td>7157.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feel</th>\n",
       "      <td>18281.0</td>\n",
       "      <td>4894.0</td>\n",
       "      <td>20432.0</td>\n",
       "      <td>2743.0</td>\n",
       "      <td>15647.0</td>\n",
       "      <td>7528.0</td>\n",
       "      <td>13952.0</td>\n",
       "      <td>9223.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>friend</th>\n",
       "      <td>7208.0</td>\n",
       "      <td>2309.0</td>\n",
       "      <td>8195.0</td>\n",
       "      <td>1322.0</td>\n",
       "      <td>5804.0</td>\n",
       "      <td>3713.0</td>\n",
       "      <td>5543.0</td>\n",
       "      <td>3974.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>friends</th>\n",
       "      <td>8952.0</td>\n",
       "      <td>3293.0</td>\n",
       "      <td>10516.0</td>\n",
       "      <td>1729.0</td>\n",
       "      <td>7286.0</td>\n",
       "      <td>4959.0</td>\n",
       "      <td>7559.0</td>\n",
       "      <td>4686.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>going</th>\n",
       "      <td>9675.0</td>\n",
       "      <td>2942.0</td>\n",
       "      <td>10834.0</td>\n",
       "      <td>1783.0</td>\n",
       "      <td>7001.0</td>\n",
       "      <td>5616.0</td>\n",
       "      <td>7553.0</td>\n",
       "      <td>5064.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>15594.0</td>\n",
       "      <td>4785.0</td>\n",
       "      <td>17567.0</td>\n",
       "      <td>2812.0</td>\n",
       "      <td>11518.0</td>\n",
       "      <td>8861.0</td>\n",
       "      <td>12365.0</td>\n",
       "      <td>8014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>got</th>\n",
       "      <td>7705.0</td>\n",
       "      <td>2516.0</td>\n",
       "      <td>8682.0</td>\n",
       "      <td>1539.0</td>\n",
       "      <td>5613.0</td>\n",
       "      <td>4608.0</td>\n",
       "      <td>6475.0</td>\n",
       "      <td>3746.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guys</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1908.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just</th>\n",
       "      <td>42324.0</td>\n",
       "      <td>13110.0</td>\n",
       "      <td>47765.0</td>\n",
       "      <td>7669.0</td>\n",
       "      <td>31144.0</td>\n",
       "      <td>24290.0</td>\n",
       "      <td>34043.0</td>\n",
       "      <td>21391.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 i        e        n       s        f        t        p  \\\n",
       "actually    9998.0   3127.0  11471.0  1654.0   7148.0   5977.0   8137.0   \n",
       "best        6955.0   2322.0   7977.0  1300.0   5312.0   3965.0   5571.0   \n",
       "better      6388.0   1888.0   7169.0     NaN   4580.0   3696.0   4909.0   \n",
       "day            NaN      NaN      NaN     NaN   4567.0      NaN      NaN   \n",
       "did        14532.0   4328.0  16222.0  2638.0  10042.0   8818.0  11189.0   \n",
       "different   6211.0      NaN   7016.0     NaN      NaN      NaN   4835.0   \n",
       "does       13277.0   3942.0  14897.0  2322.0   8817.0   8402.0  10062.0   \n",
       "feel       18281.0   4894.0  20432.0  2743.0  15647.0   7528.0  13952.0   \n",
       "friend      7208.0   2309.0   8195.0  1322.0   5804.0   3713.0   5543.0   \n",
       "friends     8952.0   3293.0  10516.0  1729.0   7286.0   4959.0   7559.0   \n",
       "going       9675.0   2942.0  10834.0  1783.0   7001.0   5616.0   7553.0   \n",
       "good       15594.0   4785.0  17567.0  2812.0  11518.0   8861.0  12365.0   \n",
       "got         7705.0   2516.0   8682.0  1539.0   5613.0   4608.0   6475.0   \n",
       "guys           NaN   1908.0      NaN     NaN      NaN      NaN      NaN   \n",
       "just       42324.0  13110.0  47765.0  7669.0  31144.0  24290.0  34043.0   \n",
       "\n",
       "                 j  \n",
       "actually    4988.0  \n",
       "best        3706.0  \n",
       "better      3367.0  \n",
       "day            NaN  \n",
       "did         7671.0  \n",
       "different      NaN  \n",
       "does        7157.0  \n",
       "feel        9223.0  \n",
       "friend      3974.0  \n",
       "friends     4686.0  \n",
       "going       5064.0  \n",
       "good        8014.0  \n",
       "got         3746.0  \n",
       "guys           NaN  \n",
       "just       21391.0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top_vocabs.columns = [list('iensftpj')]\n",
    "\n",
    "for c in df_top_vocabs:\n",
    "    df_top_vocabs[c]=df_top_vocabs[c].apply(lambda x :round (x,3),)\n",
    "\n",
    "    \n",
    "df_top_vocabs.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_vocabs.fillna(0.,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### num of entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of entries \n",
    "temp = df_merge_clean_posts[['world_E_I','num_of_entries']].groupby('world_E_I')['num_of_entries'].sum()\n",
    "num_entries_E = temp[1]\n",
    "num_entries_I = temp[0]\n",
    "temp = df_merge_clean_posts[['information_S_N','num_of_entries']].groupby('information_S_N')['num_of_entries'].sum()\n",
    "num_entries_S= temp[1]\n",
    "num_entries_N = temp[0]\n",
    "temp = df_merge_clean_posts[['decision_T_F','num_of_entries']].groupby('decision_T_F')['num_of_entries'].sum()\n",
    "num_entries_T = temp[1]\n",
    "num_entries_F = temp[0]\n",
    "temp = df_merge_clean_posts[['structure_J_P','num_of_entries']].groupby('structure_J_P')['num_of_entries'].sum()\n",
    "num_entries_J = temp[1]\n",
    "num_entries_P = temp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### vocabs use per entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_vocabs.e = df_top_vocabs.e / num_entries_E\n",
    "df_top_vocabs.i = df_top_vocabs.i / num_entries_I\n",
    "\n",
    "df_top_vocabs.s = df_top_vocabs.s / num_entries_S\n",
    "df_top_vocabs.n = df_top_vocabs.n / num_entries_N\n",
    "\n",
    "df_top_vocabs.t = df_top_vocabs.t / num_entries_T\n",
    "df_top_vocabs.f = df_top_vocabs.f / num_entries_F\n",
    "\n",
    "df_top_vocabs.j = df_top_vocabs.j / num_entries_J\n",
    "df_top_vocabs.p = df_top_vocabs.p / num_entries_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_vocabs = df_top_vocabs.apply(lambda x:round( x*100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>e</th>\n",
       "      <th>n</th>\n",
       "      <th>s</th>\n",
       "      <th>f</th>\n",
       "      <th>t</th>\n",
       "      <th>p</th>\n",
       "      <th>j</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actually</th>\n",
       "      <td>3.4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>2.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>better</th>\n",
       "      <td>2.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>did</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>different</th>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>does</th>\n",
       "      <td>4.5</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feel</th>\n",
       "      <td>6.3</td>\n",
       "      <td>5.5</td>\n",
       "      <td>6.2</td>\n",
       "      <td>5.3</td>\n",
       "      <td>7.6</td>\n",
       "      <td>4.3</td>\n",
       "      <td>6.1</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>friend</th>\n",
       "      <td>2.5</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>friends</th>\n",
       "      <td>3.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             i    e    n    s    f    t    p    j\n",
       "actually   3.4  3.5  3.5  3.2  3.5  3.4  3.5  3.3\n",
       "best       2.4  2.6  2.4  2.5  2.6  2.3  2.4  2.4\n",
       "better     2.2  2.1  2.2  0.0  2.2  2.1  2.1  2.2\n",
       "day        0.0  0.0  0.0  0.0  2.2  0.0  0.0  0.0\n",
       "did        5.0  4.9  4.9  5.1  4.9  5.1  4.9  5.1\n",
       "different  2.1  0.0  2.1  0.0  0.0  0.0  2.1  0.0\n",
       "does       4.5  4.4  4.5  4.5  4.3  4.8  4.4  4.7\n",
       "feel       6.3  5.5  6.2  5.3  7.6  4.3  6.1  6.1\n",
       "friend     2.5  2.6  2.5  2.6  2.8  2.1  2.4  2.6\n",
       "friends    3.1  3.7  3.2  3.4  3.5  2.8  3.3  3.1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top_vocabs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
